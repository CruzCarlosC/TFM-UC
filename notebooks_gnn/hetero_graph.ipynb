{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import os\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "file_path = cwd + '/points_150.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>N_side</th>\n",
       "      <th>N_layer</th>\n",
       "      <th>t_label</th>\n",
       "      <th>phi</th>\n",
       "      <th>eta</th>\n",
       "      <th>q</th>\n",
       "      <th>pt</th>\n",
       "      <th>d0</th>\n",
       "      <th>z0</th>\n",
       "      <th>n_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8856</td>\n",
       "      <td>-5.8159</td>\n",
       "      <td>-29.5959</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.3150</td>\n",
       "      <td>-12.9382</td>\n",
       "      <td>-70.4852</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.7589</td>\n",
       "      <td>-20.0473</td>\n",
       "      <td>-111.3745</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.2174</td>\n",
       "      <td>-27.1433</td>\n",
       "      <td>-152.2638</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.6903</td>\n",
       "      <td>-34.2260</td>\n",
       "      <td>-193.1531</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x        y         z  N_side  N_layer t_label     phi     eta  q  \\\n",
       "0   7.8856  -5.8159  -29.5959      10        1      T0 -0.8395 -2.1563 -1   \n",
       "1  14.3150 -12.9382  -70.4852      10        2      T0 -0.8395 -2.1563 -1   \n",
       "2  20.7589 -20.0473 -111.3745      10        3      T0 -0.8395 -2.1563 -1   \n",
       "3  27.2174 -27.1433 -152.2638      10        4      T0 -0.8395 -2.1563 -1   \n",
       "4  33.6903 -34.2260 -193.1531      10        5      T0 -0.8395 -2.1563 -1   \n",
       "\n",
       "        pt      d0      z0  n_label  \n",
       "0  54.6582  0.0198  0.1129        0  \n",
       "1  54.6582  0.0198  0.1129        1  \n",
       "2  54.6582  0.0198  0.1129        2  \n",
       "3  54.6582  0.0198  0.1129        3  \n",
       "4  54.6582  0.0198  0.1129        4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(file_path)\n",
    "\n",
    "#Round the values of the dataset to 4 decimal places\n",
    "df = df.round(4)\n",
    "\n",
    "#Add a column to use as index from 0 to the length of the dataset\n",
    "df['n_label'] = range(0, len(df))\n",
    "\n",
    "#delete the column p_label\n",
    "df = df.drop('p_label', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty hetero graph \n",
    "data=HeteroData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node names\n",
    "nodes_s=df['n_label'].values\n",
    "nodes_t=df['n_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add nodes to the graph\n",
    "data['source'].node_id = torch.tensor(nodes_s, dtype=torch.long)\n",
    "data['target'].node_id = torch.tensor(nodes_t, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add node attributes, in this case the position of the points\n",
    "data['source'].x = Tensor(df[['x', 'y', 'z']].values)\n",
    "data['target'].x = Tensor(df[['x', 'y', 'z']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>471</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>1468</td>\n",
       "      <td>209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>1468</td>\n",
       "      <td>1469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>1478</td>\n",
       "      <td>1479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>1488</td>\n",
       "      <td>1489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>1498</td>\n",
       "      <td>1499</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3538 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Source  Target  weight\n",
       "0          0     111     1.0\n",
       "1          0     161     1.0\n",
       "2          0     181     1.0\n",
       "3          0     201     1.0\n",
       "4          0     471     1.0\n",
       "...      ...     ...     ...\n",
       "3533    1468     209     0.0\n",
       "3534    1468    1469     1.0\n",
       "3535    1478    1479     1.0\n",
       "3536    1488    1489     1.0\n",
       "3537    1498    1499     1.0\n",
       "\n",
       "[3538 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_path = cwd + '/grap_150.csv'\n",
    "\n",
    "# Importing the dataset\n",
    "df_edge = pd.read_csv(edge_path)\n",
    "df_edge = df_edge.replace({'weight':0.5}, 0.)\n",
    "df_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([df_edge['Source'], df_edge['Target']], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source', 'weight', 'target'].edge_index = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={ edge_index=[2, 3538] }\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge attributes\n",
    "weight_val = torch.from_numpy(df_edge['weight'].values).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source', 'weight', 'target'].edge_label=weight_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 3538],\n",
       "    edge_label=[3538],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if the data is valid\n",
    "data.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "data = T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['target', 'rev_weight', 'source'].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 3538],\n",
       "    edge_label=[3538],\n",
       "  },\n",
       "  (target, rev_weight, source)={ edge_index=[2, 3538] }\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.validate(raise_on_error=True))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.001,\n",
    "    num_test=0.001,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('source', 'weight', 'target')],\n",
    "    rev_edge_types=[('target', 'rev_weight', 'source')],\n",
    ")(data)\n",
    "torch.save(test_data,'test_data_150.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 3535],\n",
       "    edge_label=[3],\n",
       "    edge_label_index=[2, 3],\n",
       "  },\n",
       "  (target, rev_weight, source)={ edge_index=[2, 3535] }\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (encoder): GraphModule(\n",
      "    (conv1): ModuleDict(\n",
      "      (source__weight__target): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "      (target__rev_weight__source): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "    )\n",
      "    (conv2): ModuleDict(\n",
      "      (source__weight__target): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "      (target__rev_weight__source): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "    )\n",
      "  )\n",
      "  (decoder): EdgeDecoder(\n",
      "    (lin1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (lin2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['source'][row], z_dict['target'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(hidden_channels=32).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['source', 'target'].edge_label_index)\n",
    "    target = train_data['source', 'target'].edge_label\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['source', 'target'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    target = data['source', 'target'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 46.4061, Train: 0.6941, Val: 0.5774\n",
      "Epoch: 002, Loss: 470.1983, Train: 0.7244, Val: 0.5774\n",
      "Epoch: 003, Loss: 32.1533, Train: 0.7507, Val: 0.8165\n",
      "Epoch: 004, Loss: 47.5333, Train: 0.7434, Val: 0.8165\n",
      "Epoch: 005, Loss: 47.1463, Train: 0.7117, Val: 0.8165\n",
      "Epoch: 006, Loss: 8.7103, Train: 0.6486, Val: 0.5774\n",
      "Epoch: 007, Loss: 4.8073, Train: 0.6555, Val: 0.5774\n",
      "Epoch: 008, Loss: 19.7501, Train: 0.6485, Val: 0.5774\n",
      "Epoch: 009, Loss: 17.7802, Train: 0.6414, Val: 0.5774\n",
      "Epoch: 010, Loss: 8.7994, Train: 0.6784, Val: 0.8165\n",
      "Epoch: 011, Loss: 4.9222, Train: 0.7096, Val: 0.8165\n",
      "Epoch: 012, Loss: 4.4012, Train: 0.6918, Val: 0.6851\n",
      "Epoch: 013, Loss: 3.5177, Train: 0.6779, Val: 0.8429\n",
      "Epoch: 014, Loss: 1.9968, Train: 0.6735, Val: 0.8305\n",
      "Epoch: 015, Loss: 0.7589, Train: 0.6456, Val: 0.6793\n",
      "Epoch: 016, Loss: 0.4409, Train: 0.6824, Val: 0.5805\n",
      "Epoch: 017, Loss: 0.7054, Train: 0.6957, Val: 0.5783\n",
      "Epoch: 018, Loss: 1.0547, Train: 0.7010, Val: 0.5776\n",
      "Epoch: 019, Loss: 1.1667, Train: 0.7034, Val: 0.5774\n",
      "Epoch: 020, Loss: 0.9992, Train: 0.7024, Val: 0.5774\n",
      "Epoch: 021, Loss: 0.7222, Train: 0.6867, Val: 0.6040\n",
      "Epoch: 022, Loss: 0.5211, Train: 0.6723, Val: 0.7133\n",
      "Epoch: 023, Loss: 0.4716, Train: 0.6895, Val: 0.8165\n",
      "Epoch: 024, Loss: 0.5352, Train: 0.7063, Val: 0.8165\n",
      "Epoch: 025, Loss: 0.6218, Train: 0.7126, Val: 0.8165\n",
      "Epoch: 026, Loss: 0.6709, Train: 0.7126, Val: 0.8165\n",
      "Epoch: 027, Loss: 0.6700, Train: 0.7086, Val: 0.8165\n",
      "Epoch: 028, Loss: 0.6362, Train: 0.7018, Val: 0.8165\n",
      "Epoch: 029, Loss: 0.5887, Train: 0.6932, Val: 0.8165\n",
      "Epoch: 030, Loss: 0.5410, Train: 0.6844, Val: 0.7923\n",
      "Epoch: 031, Loss: 0.5023, Train: 0.6777, Val: 0.7276\n",
      "Epoch: 032, Loss: 0.4770, Train: 0.6752, Val: 0.6724\n",
      "Epoch: 033, Loss: 0.4658, Train: 0.6777, Val: 0.6308\n",
      "Epoch: 034, Loss: 0.4664, Train: 0.6825, Val: 0.6027\n",
      "Epoch: 035, Loss: 0.4746, Train: 0.6880, Val: 0.5859\n",
      "Epoch: 036, Loss: 0.4854, Train: 0.6919, Val: 0.5770\n",
      "Epoch: 037, Loss: 0.4950, Train: 0.6936, Val: 0.5724\n",
      "Epoch: 038, Loss: 0.5006, Train: 0.6934, Val: 0.5700\n",
      "Epoch: 039, Loss: 0.5011, Train: 0.6914, Val: 0.5687\n",
      "Epoch: 040, Loss: 0.4964, Train: 0.6879, Val: 0.5686\n",
      "Epoch: 041, Loss: 0.4877, Train: 0.6828, Val: 0.5707\n",
      "Epoch: 042, Loss: 0.4763, Train: 0.6763, Val: 0.5757\n",
      "Epoch: 043, Loss: 0.4638, Train: 0.6688, Val: 0.5841\n",
      "Epoch: 044, Loss: 0.4513, Train: 0.6614, Val: 0.5907\n",
      "Epoch: 045, Loss: 0.4399, Train: 0.6547, Val: 0.5991\n",
      "Epoch: 046, Loss: 0.4303, Train: 0.6488, Val: 0.6088\n",
      "Epoch: 047, Loss: 0.4225, Train: 0.6438, Val: 0.6192\n",
      "Epoch: 048, Loss: 0.4162, Train: 0.6392, Val: 0.6295\n",
      "Epoch: 049, Loss: 0.4108, Train: 0.6343, Val: 0.6389\n",
      "Epoch: 050, Loss: 0.4054, Train: 0.6285, Val: 0.6467\n",
      "Epoch: 051, Loss: 0.3991, Train: 0.6200, Val: 0.6523\n",
      "Epoch: 052, Loss: 0.3896, Train: 0.6068, Val: 0.6553\n",
      "Epoch: 053, Loss: 0.3747, Train: 0.5857, Val: 0.6555\n",
      "Epoch: 054, Loss: 0.3518, Train: 0.5670, Val: 0.6530\n",
      "Epoch: 055, Loss: 0.3524, Train: 0.5612, Val: 0.6479\n",
      "Epoch: 056, Loss: 0.3473, Train: 0.5619, Val: 0.6406\n",
      "Epoch: 057, Loss: 0.3290, Train: 0.5676, Val: 0.6317\n",
      "Epoch: 058, Loss: 0.3328, Train: 0.5669, Val: 0.6217\n",
      "Epoch: 059, Loss: 0.3312, Train: 0.5646, Val: 0.6112\n",
      "Epoch: 060, Loss: 0.3280, Train: 0.5613, Val: 0.6006\n",
      "Epoch: 061, Loss: 0.3235, Train: 0.5569, Val: 0.5904\n",
      "Epoch: 062, Loss: 0.3179, Train: 0.5514, Val: 0.5809\n",
      "Epoch: 063, Loss: 0.3111, Train: 0.5446, Val: 0.5726\n",
      "Epoch: 064, Loss: 0.3035, Train: 0.5352, Val: 0.5654\n",
      "Epoch: 065, Loss: 0.2941, Train: 0.5167, Val: 0.5594\n",
      "Epoch: 066, Loss: 0.2789, Train: 0.5157, Val: 0.4624\n",
      "Epoch: 067, Loss: 0.2928, Train: 0.5181, Val: 0.5508\n",
      "Epoch: 068, Loss: 0.2746, Train: 0.5259, Val: 0.5482\n",
      "Epoch: 069, Loss: 0.2805, Train: 0.5197, Val: 0.5465\n",
      "Epoch: 070, Loss: 0.2743, Train: 0.5041, Val: 0.5314\n",
      "Epoch: 071, Loss: 0.2637, Train: 0.5041, Val: 0.4487\n",
      "Epoch: 072, Loss: 0.2703, Train: 0.5033, Val: 0.5467\n",
      "Epoch: 073, Loss: 0.2587, Train: 0.5090, Val: 0.5482\n",
      "Epoch: 074, Loss: 0.2627, Train: 0.4985, Val: 0.5503\n",
      "Epoch: 075, Loss: 0.2535, Train: 0.4912, Val: 0.5040\n",
      "Epoch: 076, Loss: 0.2525, Train: 0.4890, Val: 0.5110\n",
      "Epoch: 077, Loss: 0.2507, Train: 0.4886, Val: 0.5586\n",
      "Epoch: 078, Loss: 0.2443, Train: 0.4926, Val: 0.5615\n",
      "Epoch: 079, Loss: 0.2470, Train: 0.4837, Val: 0.5642\n",
      "Epoch: 080, Loss: 0.2401, Train: 0.4816, Val: 0.5863\n",
      "Epoch: 081, Loss: 0.2428, Train: 0.4785, Val: 0.5890\n",
      "Epoch: 082, Loss: 0.2375, Train: 0.4824, Val: 0.5688\n",
      "Epoch: 083, Loss: 0.2378, Train: 0.4814, Val: 0.5689\n",
      "Epoch: 084, Loss: 0.2365, Train: 0.4754, Val: 0.5863\n",
      "Epoch: 085, Loss: 0.2329, Train: 0.4760, Val: 0.6015\n",
      "Epoch: 086, Loss: 0.2346, Train: 0.4761, Val: 0.5662\n",
      "Epoch: 087, Loss: 0.2310, Train: 0.4780, Val: 0.5631\n",
      "Epoch: 088, Loss: 0.2320, Train: 0.4728, Val: 0.5776\n",
      "Epoch: 089, Loss: 0.2282, Train: 0.4733, Val: 0.5943\n",
      "Epoch: 090, Loss: 0.2297, Train: 0.4725, Val: 0.5639\n",
      "Epoch: 091, Loss: 0.2266, Train: 0.4734, Val: 0.5571\n",
      "Epoch: 092, Loss: 0.2270, Train: 0.4698, Val: 0.5808\n",
      "Epoch: 093, Loss: 0.2247, Train: 0.4695, Val: 0.5925\n",
      "Epoch: 094, Loss: 0.2249, Train: 0.4693, Val: 0.5652\n",
      "Epoch: 095, Loss: 0.2231, Train: 0.4693, Val: 0.5621\n",
      "Epoch: 096, Loss: 0.2230, Train: 0.4667, Val: 0.5858\n",
      "Epoch: 097, Loss: 0.2215, Train: 0.4663, Val: 0.5950\n",
      "Epoch: 098, Loss: 0.2215, Train: 0.4665, Val: 0.5698\n",
      "Epoch: 099, Loss: 0.2205, Train: 0.4662, Val: 0.5682\n",
      "Epoch: 100, Loss: 0.2201, Train: 0.4643, Val: 0.5886\n",
      "Epoch: 101, Loss: 0.2191, Train: 0.4638, Val: 0.5915\n",
      "Epoch: 102, Loss: 0.2187, Train: 0.4641, Val: 0.5714\n",
      "Epoch: 103, Loss: 0.2180, Train: 0.4636, Val: 0.5703\n",
      "Epoch: 104, Loss: 0.2174, Train: 0.4624, Val: 0.5866\n",
      "Epoch: 105, Loss: 0.2169, Train: 0.4620, Val: 0.5843\n",
      "Epoch: 106, Loss: 0.2163, Train: 0.4624, Val: 0.5674\n",
      "Epoch: 107, Loss: 0.2160, Train: 0.4619, Val: 0.5681\n",
      "Epoch: 108, Loss: 0.2154, Train: 0.4609, Val: 0.5814\n",
      "Epoch: 109, Loss: 0.2150, Train: 0.4605, Val: 0.5796\n",
      "Epoch: 110, Loss: 0.2146, Train: 0.4607, Val: 0.5661\n",
      "Epoch: 111, Loss: 0.2142, Train: 0.4602, Val: 0.5666\n",
      "Epoch: 112, Loss: 0.2138, Train: 0.4594, Val: 0.5782\n",
      "Epoch: 113, Loss: 0.2135, Train: 0.4591, Val: 0.5747\n",
      "Epoch: 114, Loss: 0.2130, Train: 0.4593, Val: 0.5633\n",
      "Epoch: 115, Loss: 0.2129, Train: 0.4587, Val: 0.5662\n",
      "Epoch: 116, Loss: 0.2123, Train: 0.4581, Val: 0.5765\n",
      "Epoch: 117, Loss: 0.2122, Train: 0.4578, Val: 0.5706\n",
      "Epoch: 118, Loss: 0.2116, Train: 0.4581, Val: 0.5602\n",
      "Epoch: 119, Loss: 0.2115, Train: 0.4572, Val: 0.5663\n",
      "Epoch: 120, Loss: 0.2109, Train: 0.4569, Val: 0.5735\n",
      "Epoch: 121, Loss: 0.2108, Train: 0.4566, Val: 0.5666\n",
      "Epoch: 122, Loss: 0.2103, Train: 0.4568, Val: 0.5588\n",
      "Epoch: 123, Loss: 0.2102, Train: 0.4561, Val: 0.5652\n",
      "Epoch: 124, Loss: 0.2097, Train: 0.4558, Val: 0.5728\n",
      "Epoch: 125, Loss: 0.2097, Train: 0.4556, Val: 0.5637\n",
      "Epoch: 126, Loss: 0.2092, Train: 0.4555, Val: 0.5597\n",
      "Epoch: 127, Loss: 0.2090, Train: 0.4549, Val: 0.5657\n",
      "Epoch: 128, Loss: 0.2086, Train: 0.4546, Val: 0.5698\n",
      "Epoch: 129, Loss: 0.2084, Train: 0.4544, Val: 0.5633\n",
      "Epoch: 130, Loss: 0.2081, Train: 0.4543, Val: 0.5606\n",
      "Epoch: 131, Loss: 0.2079, Train: 0.4537, Val: 0.5663\n",
      "Epoch: 132, Loss: 0.2076, Train: 0.4535, Val: 0.5667\n",
      "Epoch: 133, Loss: 0.2073, Train: 0.4533, Val: 0.5612\n",
      "Epoch: 134, Loss: 0.2070, Train: 0.4531, Val: 0.5602\n",
      "Epoch: 135, Loss: 0.2068, Train: 0.4527, Val: 0.5629\n",
      "Epoch: 136, Loss: 0.2065, Train: 0.4524, Val: 0.5640\n",
      "Epoch: 137, Loss: 0.2063, Train: 0.4522, Val: 0.5596\n",
      "Epoch: 138, Loss: 0.2060, Train: 0.4521, Val: 0.5570\n",
      "Epoch: 139, Loss: 0.2058, Train: 0.4517, Val: 0.5601\n",
      "Epoch: 140, Loss: 0.2055, Train: 0.4514, Val: 0.5602\n",
      "Epoch: 141, Loss: 0.2053, Train: 0.4512, Val: 0.5573\n",
      "Epoch: 142, Loss: 0.2050, Train: 0.4510, Val: 0.5563\n",
      "Epoch: 143, Loss: 0.2048, Train: 0.4506, Val: 0.5588\n",
      "Epoch: 144, Loss: 0.2046, Train: 0.4504, Val: 0.5579\n",
      "Epoch: 145, Loss: 0.2043, Train: 0.4502, Val: 0.5551\n",
      "Epoch: 146, Loss: 0.2041, Train: 0.4499, Val: 0.5573\n",
      "Epoch: 147, Loss: 0.2039, Train: 0.4496, Val: 0.5586\n",
      "Epoch: 148, Loss: 0.2037, Train: 0.4495, Val: 0.5544\n",
      "Epoch: 149, Loss: 0.2035, Train: 0.4492, Val: 0.5553\n",
      "Epoch: 150, Loss: 0.2032, Train: 0.4489, Val: 0.5581\n",
      "Epoch: 151, Loss: 0.2030, Train: 0.4488, Val: 0.5554\n",
      "Epoch: 152, Loss: 0.2028, Train: 0.4485, Val: 0.5556\n",
      "Epoch: 153, Loss: 0.2026, Train: 0.4483, Val: 0.5578\n",
      "Epoch: 154, Loss: 0.2024, Train: 0.4481, Val: 0.5556\n",
      "Epoch: 155, Loss: 0.2022, Train: 0.4479, Val: 0.5552\n",
      "Epoch: 156, Loss: 0.2020, Train: 0.4476, Val: 0.5584\n",
      "Epoch: 157, Loss: 0.2018, Train: 0.4474, Val: 0.5553\n",
      "Epoch: 158, Loss: 0.2016, Train: 0.4472, Val: 0.5556\n",
      "Epoch: 159, Loss: 0.2013, Train: 0.4469, Val: 0.5575\n",
      "Epoch: 160, Loss: 0.2011, Train: 0.4467, Val: 0.5557\n",
      "Epoch: 161, Loss: 0.2009, Train: 0.4465, Val: 0.5558\n",
      "Epoch: 162, Loss: 0.2007, Train: 0.4463, Val: 0.5573\n",
      "Epoch: 163, Loss: 0.2005, Train: 0.4461, Val: 0.5558\n",
      "Epoch: 164, Loss: 0.2003, Train: 0.4459, Val: 0.5559\n",
      "Epoch: 165, Loss: 0.2001, Train: 0.4456, Val: 0.5573\n",
      "Epoch: 166, Loss: 0.1999, Train: 0.4455, Val: 0.5560\n",
      "Epoch: 167, Loss: 0.1997, Train: 0.4453, Val: 0.5561\n",
      "Epoch: 168, Loss: 0.1995, Train: 0.4450, Val: 0.5587\n",
      "Epoch: 169, Loss: 0.1994, Train: 0.4449, Val: 0.5565\n",
      "Epoch: 170, Loss: 0.1992, Train: 0.4447, Val: 0.5569\n",
      "Epoch: 171, Loss: 0.1990, Train: 0.4445, Val: 0.5589\n",
      "Epoch: 172, Loss: 0.1988, Train: 0.4443, Val: 0.5571\n",
      "Epoch: 173, Loss: 0.1987, Train: 0.4441, Val: 0.5575\n",
      "Epoch: 174, Loss: 0.1985, Train: 0.4439, Val: 0.5591\n",
      "Epoch: 175, Loss: 0.1983, Train: 0.4437, Val: 0.5580\n",
      "Epoch: 176, Loss: 0.1981, Train: 0.4435, Val: 0.5585\n",
      "Epoch: 177, Loss: 0.1980, Train: 0.4433, Val: 0.5589\n",
      "Epoch: 178, Loss: 0.1978, Train: 0.4431, Val: 0.5588\n",
      "Epoch: 179, Loss: 0.1976, Train: 0.4428, Val: 0.5582\n",
      "Epoch: 180, Loss: 0.1974, Train: 0.4426, Val: 0.5589\n",
      "Epoch: 181, Loss: 0.1972, Train: 0.4424, Val: 0.5574\n",
      "Epoch: 182, Loss: 0.1970, Train: 0.4422, Val: 0.5566\n",
      "Epoch: 183, Loss: 0.1968, Train: 0.4419, Val: 0.5586\n",
      "Epoch: 184, Loss: 0.1966, Train: 0.4418, Val: 0.5559\n",
      "Epoch: 185, Loss: 0.1965, Train: 0.4416, Val: 0.5561\n",
      "Epoch: 186, Loss: 0.1963, Train: 0.4414, Val: 0.5576\n",
      "Epoch: 187, Loss: 0.1961, Train: 0.4413, Val: 0.5561\n",
      "Epoch: 188, Loss: 0.1959, Train: 0.4411, Val: 0.5561\n",
      "Epoch: 189, Loss: 0.1957, Train: 0.4408, Val: 0.5590\n",
      "Epoch: 190, Loss: 0.1955, Train: 0.4406, Val: 0.5559\n",
      "Epoch: 191, Loss: 0.1953, Train: 0.4404, Val: 0.5580\n",
      "Epoch: 192, Loss: 0.1952, Train: 0.4402, Val: 0.5577\n",
      "Epoch: 193, Loss: 0.1950, Train: 0.4400, Val: 0.5568\n",
      "Epoch: 194, Loss: 0.1948, Train: 0.4398, Val: 0.5593\n",
      "Epoch: 195, Loss: 0.1946, Train: 0.4396, Val: 0.5563\n",
      "Epoch: 196, Loss: 0.1944, Train: 0.4394, Val: 0.5575\n",
      "Epoch: 197, Loss: 0.1942, Train: 0.4392, Val: 0.5581\n",
      "Epoch: 198, Loss: 0.1941, Train: 0.4390, Val: 0.5559\n",
      "Epoch: 199, Loss: 0.1939, Train: 0.4388, Val: 0.5562\n",
      "Epoch: 200, Loss: 0.1937, Train: 0.4386, Val: 0.5557\n",
      "Epoch: 201, Loss: 0.1935, Train: 0.4384, Val: 0.5548\n",
      "Epoch: 202, Loss: 0.1933, Train: 0.4382, Val: 0.5542\n",
      "Epoch: 203, Loss: 0.1932, Train: 0.4381, Val: 0.5551\n",
      "Epoch: 204, Loss: 0.1930, Train: 0.4379, Val: 0.5541\n",
      "Epoch: 205, Loss: 0.1928, Train: 0.4377, Val: 0.5521\n",
      "Epoch: 206, Loss: 0.1926, Train: 0.4375, Val: 0.5572\n",
      "Epoch: 207, Loss: 0.1925, Train: 0.4375, Val: 0.5494\n",
      "Epoch: 208, Loss: 0.1924, Train: 0.4371, Val: 0.5568\n",
      "Epoch: 209, Loss: 0.1922, Train: 0.4370, Val: 0.5518\n",
      "Epoch: 210, Loss: 0.1920, Train: 0.4367, Val: 0.5537\n",
      "Epoch: 211, Loss: 0.1918, Train: 0.4365, Val: 0.5534\n",
      "Epoch: 212, Loss: 0.1916, Train: 0.4364, Val: 0.5519\n",
      "Epoch: 213, Loss: 0.1915, Train: 0.4362, Val: 0.5543\n",
      "Epoch: 214, Loss: 0.1913, Train: 0.4360, Val: 0.5510\n",
      "Epoch: 215, Loss: 0.1911, Train: 0.4358, Val: 0.5544\n",
      "Epoch: 216, Loss: 0.1910, Train: 0.4357, Val: 0.5496\n",
      "Epoch: 217, Loss: 0.1908, Train: 0.4354, Val: 0.5549\n",
      "Epoch: 218, Loss: 0.1907, Train: 0.4353, Val: 0.5493\n",
      "Epoch: 219, Loss: 0.1905, Train: 0.4350, Val: 0.5530\n",
      "Epoch: 220, Loss: 0.1903, Train: 0.4348, Val: 0.5520\n",
      "Epoch: 221, Loss: 0.1901, Train: 0.4346, Val: 0.5501\n",
      "Epoch: 222, Loss: 0.1900, Train: 0.4344, Val: 0.5533\n",
      "Epoch: 223, Loss: 0.1898, Train: 0.4343, Val: 0.5498\n",
      "Epoch: 224, Loss: 0.1897, Train: 0.4341, Val: 0.5534\n",
      "Epoch: 225, Loss: 0.1895, Train: 0.4339, Val: 0.5498\n",
      "Epoch: 226, Loss: 0.1893, Train: 0.4337, Val: 0.5521\n",
      "Epoch: 227, Loss: 0.1892, Train: 0.4335, Val: 0.5530\n",
      "Epoch: 228, Loss: 0.1890, Train: 0.4334, Val: 0.5495\n",
      "Epoch: 229, Loss: 0.1889, Train: 0.4332, Val: 0.5559\n",
      "Epoch: 230, Loss: 0.1887, Train: 0.4332, Val: 0.5465\n",
      "Epoch: 231, Loss: 0.1887, Train: 0.4330, Val: 0.5622\n",
      "Epoch: 232, Loss: 0.1887, Train: 0.4339, Val: 0.5382\n",
      "Epoch: 233, Loss: 0.1891, Train: 0.4339, Val: 0.5704\n",
      "Epoch: 234, Loss: 0.1898, Train: 0.4367, Val: 0.5263\n",
      "Epoch: 235, Loss: 0.1914, Train: 0.4337, Val: 0.5661\n",
      "Epoch: 236, Loss: 0.1896, Train: 0.4337, Val: 0.5345\n",
      "Epoch: 237, Loss: 0.1890, Train: 0.4321, Val: 0.5620\n",
      "Epoch: 238, Loss: 0.1880, Train: 0.4320, Val: 0.5430\n",
      "Epoch: 239, Loss: 0.1877, Train: 0.4316, Val: 0.5559\n",
      "Epoch: 240, Loss: 0.1874, Train: 0.4315, Val: 0.5464\n",
      "Epoch: 241, Loss: 0.1872, Train: 0.4312, Val: 0.5517\n",
      "Epoch: 242, Loss: 0.1870, Train: 0.4310, Val: 0.5507\n",
      "Epoch: 243, Loss: 0.1869, Train: 0.4309, Val: 0.5474\n",
      "Epoch: 244, Loss: 0.1868, Train: 0.4307, Val: 0.5579\n",
      "Epoch: 245, Loss: 0.1868, Train: 0.4316, Val: 0.5388\n",
      "Epoch: 246, Loss: 0.1872, Train: 0.4327, Val: 0.5682\n",
      "Epoch: 247, Loss: 0.1890, Train: 0.4408, Val: 0.5251\n",
      "Epoch: 248, Loss: 0.1949, Train: 0.4304, Val: 0.5640\n",
      "Epoch: 249, Loss: 0.1866, Train: 0.4313, Val: 0.5660\n",
      "Epoch: 250, Loss: 0.1877, Train: 0.4419, Val: 0.5249\n",
      "Epoch: 251, Loss: 0.1958, Train: 0.4298, Val: 0.5463\n",
      "Epoch: 252, Loss: 0.1859, Train: 0.4432, Val: 0.5213\n",
      "Epoch: 253, Loss: 0.2004, Train: 0.4586, Val: 0.5247\n",
      "Epoch: 254, Loss: 0.2105, Train: 0.4715, Val: 0.5247\n",
      "Epoch: 255, Loss: 0.2223, Train: 0.4764, Val: 0.5250\n",
      "Epoch: 256, Loss: 0.2269, Train: 0.4727, Val: 0.5255\n",
      "Epoch: 257, Loss: 0.2235, Train: 0.4626, Val: 0.5262\n",
      "Epoch: 258, Loss: 0.2142, Train: 0.4641, Val: 0.5523\n",
      "Epoch: 259, Loss: 0.2167, Train: 0.4607, Val: 0.5396\n",
      "Epoch: 260, Loss: 0.2136, Train: 0.4536, Val: 0.5259\n",
      "Epoch: 261, Loss: 0.2062, Train: 0.4549, Val: 0.5251\n",
      "Epoch: 262, Loss: 0.2073, Train: 0.4532, Val: 0.5246\n",
      "Epoch: 263, Loss: 0.2059, Train: 0.4490, Val: 0.5243\n",
      "Epoch: 264, Loss: 0.2027, Train: 0.4430, Val: 0.5241\n",
      "Epoch: 265, Loss: 0.1990, Train: 0.4348, Val: 0.5239\n",
      "Epoch: 266, Loss: 0.1978, Train: 0.4376, Val: 0.5323\n",
      "Epoch: 267, Loss: 0.2028, Train: 0.4416, Val: 0.5236\n",
      "Epoch: 268, Loss: 0.1983, Train: 0.4468, Val: 0.5236\n",
      "Epoch: 269, Loss: 0.2012, Train: 0.4497, Val: 0.5236\n",
      "Epoch: 270, Loss: 0.2031, Train: 0.4504, Val: 0.5238\n",
      "Epoch: 271, Loss: 0.2036, Train: 0.4495, Val: 0.5240\n",
      "Epoch: 272, Loss: 0.2029, Train: 0.4473, Val: 0.5244\n",
      "Epoch: 273, Loss: 0.2013, Train: 0.4442, Val: 0.5248\n",
      "Epoch: 274, Loss: 0.1993, Train: 0.4413, Val: 0.5251\n",
      "Epoch: 275, Loss: 0.1982, Train: 0.4383, Val: 0.5254\n",
      "Epoch: 276, Loss: 0.1974, Train: 0.4342, Val: 0.5255\n",
      "Epoch: 277, Loss: 0.1967, Train: 0.4333, Val: 0.5254\n",
      "Epoch: 278, Loss: 0.1983, Train: 0.4325, Val: 0.5251\n",
      "Epoch: 279, Loss: 0.1935, Train: 0.4346, Val: 0.5247\n",
      "Epoch: 280, Loss: 0.1927, Train: 0.4330, Val: 0.5242\n",
      "Epoch: 281, Loss: 0.1905, Train: 0.4330, Val: 0.5238\n",
      "Epoch: 282, Loss: 0.1917, Train: 0.4321, Val: 0.5234\n",
      "Epoch: 283, Loss: 0.1891, Train: 0.4347, Val: 0.5231\n",
      "Epoch: 284, Loss: 0.1901, Train: 0.4346, Val: 0.5230\n",
      "Epoch: 285, Loss: 0.1899, Train: 0.4331, Val: 0.5229\n",
      "Epoch: 286, Loss: 0.1890, Train: 0.4323, Val: 0.5230\n",
      "Epoch: 287, Loss: 0.1882, Train: 0.4327, Val: 0.5232\n",
      "Epoch: 288, Loss: 0.1881, Train: 0.4307, Val: 0.5234\n",
      "Epoch: 289, Loss: 0.1865, Train: 0.4309, Val: 0.5631\n",
      "Epoch: 290, Loss: 0.1875, Train: 0.4300, Val: 0.5258\n",
      "Epoch: 291, Loss: 0.1860, Train: 0.4289, Val: 0.5364\n",
      "Epoch: 292, Loss: 0.1853, Train: 0.4293, Val: 0.5602\n",
      "Epoch: 293, Loss: 0.1866, Train: 0.4295, Val: 0.5285\n",
      "Epoch: 294, Loss: 0.1858, Train: 0.4280, Val: 0.5328\n",
      "Epoch: 295, Loss: 0.1846, Train: 0.4280, Val: 0.5706\n",
      "Epoch: 296, Loss: 0.1859, Train: 0.4276, Val: 0.5259\n",
      "Epoch: 297, Loss: 0.1842, Train: 0.4254, Val: 0.5260\n",
      "Epoch: 298, Loss: 0.1824, Train: 0.4262, Val: 0.5591\n",
      "Epoch: 299, Loss: 0.1840, Train: 0.4253, Val: 0.5261\n",
      "Epoch: 300, Loss: 0.1822, Train: 0.4264, Val: 0.5261\n",
      "Epoch: 301, Loss: 0.1830, Train: 0.4240, Val: 0.5269\n",
      "Epoch: 302, Loss: 0.1814, Train: 0.4252, Val: 0.5391\n",
      "Epoch: 303, Loss: 0.1826, Train: 0.4255, Val: 0.5259\n",
      "Epoch: 304, Loss: 0.1821, Train: 0.4262, Val: 0.5257\n",
      "Epoch: 305, Loss: 0.1827, Train: 0.4234, Val: 0.5299\n",
      "Epoch: 306, Loss: 0.1808, Train: 0.4245, Val: 0.5453\n",
      "Epoch: 307, Loss: 0.1819, Train: 0.4248, Val: 0.5250\n",
      "Epoch: 308, Loss: 0.1813, Train: 0.4247, Val: 0.5248\n",
      "Epoch: 309, Loss: 0.1812, Train: 0.4238, Val: 0.5571\n",
      "Epoch: 310, Loss: 0.1811, Train: 0.4221, Val: 0.5406\n",
      "Epoch: 311, Loss: 0.1792, Train: 0.4235, Val: 0.5241\n",
      "Epoch: 312, Loss: 0.1802, Train: 0.4217, Val: 0.5415\n",
      "Epoch: 313, Loss: 0.1788, Train: 0.4228, Val: 0.5610\n",
      "Epoch: 314, Loss: 0.1801, Train: 0.4240, Val: 0.5234\n",
      "Epoch: 315, Loss: 0.1806, Train: 0.4212, Val: 0.5408\n",
      "Epoch: 316, Loss: 0.1784, Train: 0.4239, Val: 0.5667\n",
      "Epoch: 317, Loss: 0.1813, Train: 0.4275, Val: 0.5229\n",
      "Epoch: 318, Loss: 0.1834, Train: 0.4218, Val: 0.5265\n",
      "Epoch: 319, Loss: 0.1788, Train: 0.4299, Val: 0.5560\n",
      "Epoch: 320, Loss: 0.1873, Train: 0.4338, Val: 0.5225\n",
      "Epoch: 321, Loss: 0.1887, Train: 0.4403, Val: 0.5225\n",
      "Epoch: 322, Loss: 0.1943, Train: 0.4394, Val: 0.5226\n",
      "Epoch: 323, Loss: 0.1935, Train: 0.4350, Val: 0.5229\n",
      "Epoch: 324, Loss: 0.1900, Train: 0.4332, Val: 0.5231\n",
      "Epoch: 325, Loss: 0.1890, Train: 0.4259, Val: 0.5228\n",
      "Epoch: 326, Loss: 0.1837, Train: 0.4210, Val: 0.5363\n",
      "Epoch: 327, Loss: 0.1792, Train: 0.4230, Val: 0.5214\n",
      "Epoch: 328, Loss: 0.1799, Train: 0.4257, Val: 0.5210\n",
      "Epoch: 329, Loss: 0.1820, Train: 0.4256, Val: 0.5208\n",
      "Epoch: 330, Loss: 0.1820, Train: 0.4261, Val: 0.5256\n",
      "Epoch: 331, Loss: 0.1825, Train: 0.4265, Val: 0.5212\n",
      "Epoch: 332, Loss: 0.1825, Train: 0.4250, Val: 0.5217\n",
      "Epoch: 333, Loss: 0.1813, Train: 0.4248, Val: 0.5478\n",
      "Epoch: 334, Loss: 0.1816, Train: 0.4244, Val: 0.5230\n",
      "Epoch: 335, Loss: 0.1810, Train: 0.4230, Val: 0.5319\n",
      "Epoch: 336, Loss: 0.1801, Train: 0.4242, Val: 0.5681\n",
      "Epoch: 337, Loss: 0.1822, Train: 0.4265, Val: 0.5248\n",
      "Epoch: 338, Loss: 0.1831, Train: 0.4254, Val: 0.5253\n",
      "Epoch: 339, Loss: 0.1824, Train: 0.4212, Val: 0.5586\n",
      "Epoch: 340, Loss: 0.1806, Train: 0.4197, Val: 0.5406\n",
      "Epoch: 341, Loss: 0.1792, Train: 0.4214, Val: 0.5261\n",
      "Epoch: 342, Loss: 0.1798, Train: 0.4196, Val: 0.5261\n",
      "Epoch: 343, Loss: 0.1785, Train: 0.4202, Val: 0.5402\n",
      "Epoch: 344, Loss: 0.1799, Train: 0.4190, Val: 0.5260\n",
      "Epoch: 345, Loss: 0.1777, Train: 0.4208, Val: 0.5260\n",
      "Epoch: 346, Loss: 0.1787, Train: 0.4191, Val: 0.5260\n",
      "Epoch: 347, Loss: 0.1777, Train: 0.4208, Val: 0.5260\n",
      "Epoch: 348, Loss: 0.1794, Train: 0.4207, Val: 0.5260\n",
      "Epoch: 349, Loss: 0.1783, Train: 0.4221, Val: 0.5261\n",
      "Epoch: 350, Loss: 0.1794, Train: 0.4186, Val: 0.5262\n",
      "Epoch: 351, Loss: 0.1771, Train: 0.4203, Val: 0.5358\n",
      "Epoch: 352, Loss: 0.1790, Train: 0.4202, Val: 0.5262\n",
      "Epoch: 353, Loss: 0.1779, Train: 0.4205, Val: 0.5262\n",
      "Epoch: 354, Loss: 0.1781, Train: 0.4183, Val: 0.5384\n",
      "Epoch: 355, Loss: 0.1770, Train: 0.4173, Val: 0.5331\n",
      "Epoch: 356, Loss: 0.1760, Train: 0.4193, Val: 0.5255\n",
      "Epoch: 357, Loss: 0.1770, Train: 0.4168, Val: 0.5252\n",
      "Epoch: 358, Loss: 0.1751, Train: 0.4190, Val: 0.5589\n",
      "Epoch: 359, Loss: 0.1776, Train: 0.4199, Val: 0.5245\n",
      "Epoch: 360, Loss: 0.1774, Train: 0.4175, Val: 0.5243\n",
      "Epoch: 361, Loss: 0.1755, Train: 0.4224, Val: 0.5569\n",
      "Epoch: 362, Loss: 0.1809, Train: 0.4236, Val: 0.5240\n",
      "Epoch: 363, Loss: 0.1803, Train: 0.4260, Val: 0.5239\n",
      "Epoch: 364, Loss: 0.1823, Train: 0.4172, Val: 0.5240\n",
      "Epoch: 365, Loss: 0.1752, Train: 0.4331, Val: 0.5315\n",
      "Epoch: 366, Loss: 0.1917, Train: 0.4309, Val: 0.5236\n",
      "Epoch: 367, Loss: 0.1863, Train: 0.4406, Val: 0.5234\n",
      "Epoch: 368, Loss: 0.1946, Train: 0.4440, Val: 0.5234\n",
      "Epoch: 369, Loss: 0.1976, Train: 0.4423, Val: 0.5237\n",
      "Epoch: 370, Loss: 0.1961, Train: 0.4369, Val: 0.5242\n",
      "Epoch: 371, Loss: 0.1914, Train: 0.4351, Val: 0.5246\n",
      "Epoch: 372, Loss: 0.1901, Train: 0.4387, Val: 0.5291\n",
      "Epoch: 373, Loss: 0.1940, Train: 0.4299, Val: 0.5240\n",
      "Epoch: 374, Loss: 0.1861, Train: 0.4277, Val: 0.5232\n",
      "Epoch: 375, Loss: 0.1842, Train: 0.4263, Val: 0.5225\n",
      "Epoch: 376, Loss: 0.1831, Train: 0.4200, Val: 0.5221\n",
      "Epoch: 377, Loss: 0.1784, Train: 0.4231, Val: 0.5149\n",
      "Epoch: 378, Loss: 0.1844, Train: 0.4195, Val: 0.5220\n",
      "Epoch: 379, Loss: 0.1787, Train: 0.4236, Val: 0.5222\n",
      "Epoch: 380, Loss: 0.1809, Train: 0.4258, Val: 0.5226\n",
      "Epoch: 381, Loss: 0.1825, Train: 0.4215, Val: 0.5230\n",
      "Epoch: 382, Loss: 0.1792, Train: 0.4208, Val: 0.5235\n",
      "Epoch: 383, Loss: 0.1798, Train: 0.4195, Val: 0.5239\n",
      "Epoch: 384, Loss: 0.1784, Train: 0.4212, Val: 0.5243\n",
      "Epoch: 385, Loss: 0.1789, Train: 0.4218, Val: 0.5246\n",
      "Epoch: 386, Loss: 0.1794, Train: 0.4174, Val: 0.5249\n",
      "Epoch: 387, Loss: 0.1764, Train: 0.4202, Val: 0.5330\n",
      "Epoch: 388, Loss: 0.1805, Train: 0.4162, Val: 0.5252\n",
      "Epoch: 389, Loss: 0.1757, Train: 0.4192, Val: 0.5252\n",
      "Epoch: 390, Loss: 0.1777, Train: 0.4172, Val: 0.5253\n",
      "Epoch: 391, Loss: 0.1762, Train: 0.4159, Val: 0.5255\n",
      "Epoch: 392, Loss: 0.1762, Train: 0.4147, Val: 0.5255\n",
      "Epoch: 393, Loss: 0.1748, Train: 0.4164, Val: 0.5256\n",
      "Epoch: 394, Loss: 0.1755, Train: 0.4158, Val: 0.5257\n",
      "Epoch: 395, Loss: 0.1749, Train: 0.4147, Val: 0.5257\n",
      "Epoch: 396, Loss: 0.1746, Train: 0.4143, Val: 0.5257\n",
      "Epoch: 397, Loss: 0.1740, Train: 0.4155, Val: 0.5256\n",
      "Epoch: 398, Loss: 0.1744, Train: 0.4146, Val: 0.5255\n",
      "Epoch: 399, Loss: 0.1736, Train: 0.4143, Val: 0.5254\n",
      "Epoch: 400, Loss: 0.1737, Train: 0.4138, Val: 0.5253\n",
      "Epoch: 401, Loss: 0.1730, Train: 0.4145, Val: 0.5252\n",
      "Epoch: 402, Loss: 0.1734, Train: 0.4133, Val: 0.5251\n",
      "Epoch: 403, Loss: 0.1725, Train: 0.4135, Val: 0.5251\n",
      "Epoch: 404, Loss: 0.1729, Train: 0.4130, Val: 0.5251\n",
      "Epoch: 405, Loss: 0.1722, Train: 0.4131, Val: 0.5251\n",
      "Epoch: 406, Loss: 0.1722, Train: 0.4126, Val: 0.5251\n",
      "Epoch: 407, Loss: 0.1720, Train: 0.4122, Val: 0.5251\n",
      "Epoch: 408, Loss: 0.1716, Train: 0.4127, Val: 0.5251\n",
      "Epoch: 409, Loss: 0.1717, Train: 0.4119, Val: 0.5250\n",
      "Epoch: 410, Loss: 0.1712, Train: 0.4121, Val: 0.5261\n",
      "Epoch: 411, Loss: 0.1714, Train: 0.4122, Val: 0.5249\n",
      "Epoch: 412, Loss: 0.1712, Train: 0.4117, Val: 0.5248\n",
      "Epoch: 413, Loss: 0.1709, Train: 0.4118, Val: 0.5285\n",
      "Epoch: 414, Loss: 0.1711, Train: 0.4117, Val: 0.5246\n",
      "Epoch: 415, Loss: 0.1708, Train: 0.4113, Val: 0.5245\n",
      "Epoch: 416, Loss: 0.1706, Train: 0.4112, Val: 0.5277\n",
      "Epoch: 417, Loss: 0.1706, Train: 0.4110, Val: 0.5243\n",
      "Epoch: 418, Loss: 0.1704, Train: 0.4108, Val: 0.5242\n",
      "Epoch: 419, Loss: 0.1702, Train: 0.4106, Val: 0.5268\n",
      "Epoch: 420, Loss: 0.1703, Train: 0.4105, Val: 0.5239\n",
      "Epoch: 421, Loss: 0.1700, Train: 0.4103, Val: 0.5237\n",
      "Epoch: 422, Loss: 0.1699, Train: 0.4101, Val: 0.5235\n",
      "Epoch: 423, Loss: 0.1698, Train: 0.4101, Val: 0.5233\n",
      "Epoch: 424, Loss: 0.1697, Train: 0.4099, Val: 0.5232\n",
      "Epoch: 425, Loss: 0.1696, Train: 0.4098, Val: 0.5230\n",
      "Epoch: 426, Loss: 0.1694, Train: 0.4098, Val: 0.5229\n",
      "Epoch: 427, Loss: 0.1694, Train: 0.4094, Val: 0.5229\n",
      "Epoch: 428, Loss: 0.1693, Train: 0.4093, Val: 0.5228\n",
      "Epoch: 429, Loss: 0.1691, Train: 0.4093, Val: 0.5227\n",
      "Epoch: 430, Loss: 0.1691, Train: 0.4090, Val: 0.5248\n",
      "Epoch: 431, Loss: 0.1691, Train: 0.4089, Val: 0.5226\n",
      "Epoch: 432, Loss: 0.1689, Train: 0.4087, Val: 0.5226\n",
      "Epoch: 433, Loss: 0.1688, Train: 0.4085, Val: 0.5226\n",
      "Epoch: 434, Loss: 0.1687, Train: 0.4084, Val: 0.5226\n",
      "Epoch: 435, Loss: 0.1686, Train: 0.4082, Val: 0.5227\n",
      "Epoch: 436, Loss: 0.1684, Train: 0.4080, Val: 0.5227\n",
      "Epoch: 437, Loss: 0.1684, Train: 0.4080, Val: 0.5228\n",
      "Epoch: 438, Loss: 0.1683, Train: 0.4077, Val: 0.5228\n",
      "Epoch: 439, Loss: 0.1682, Train: 0.4076, Val: 0.5228\n",
      "Epoch: 440, Loss: 0.1681, Train: 0.4077, Val: 0.5228\n",
      "Epoch: 441, Loss: 0.1680, Train: 0.4074, Val: 0.5229\n",
      "Epoch: 442, Loss: 0.1679, Train: 0.4073, Val: 0.5229\n",
      "Epoch: 443, Loss: 0.1678, Train: 0.4071, Val: 0.5229\n",
      "Epoch: 444, Loss: 0.1677, Train: 0.4070, Val: 0.5229\n",
      "Epoch: 445, Loss: 0.1677, Train: 0.4071, Val: 0.5229\n",
      "Epoch: 446, Loss: 0.1676, Train: 0.4068, Val: 0.5230\n",
      "Epoch: 447, Loss: 0.1675, Train: 0.4067, Val: 0.5230\n",
      "Epoch: 448, Loss: 0.1675, Train: 0.4072, Val: 0.5230\n",
      "Epoch: 449, Loss: 0.1676, Train: 0.4066, Val: 0.5230\n",
      "Epoch: 450, Loss: 0.1674, Train: 0.4064, Val: 0.5230\n",
      "Epoch: 451, Loss: 0.1671, Train: 0.4068, Val: 0.5231\n",
      "Epoch: 452, Loss: 0.1673, Train: 0.4062, Val: 0.5232\n",
      "Epoch: 453, Loss: 0.1671, Train: 0.4061, Val: 0.5232\n",
      "Epoch: 454, Loss: 0.1669, Train: 0.4061, Val: 0.5233\n",
      "Epoch: 455, Loss: 0.1668, Train: 0.4059, Val: 0.5233\n",
      "Epoch: 456, Loss: 0.1668, Train: 0.4061, Val: 0.5234\n",
      "Epoch: 457, Loss: 0.1668, Train: 0.4058, Val: 0.5235\n",
      "Epoch: 458, Loss: 0.1666, Train: 0.4057, Val: 0.5235\n",
      "Epoch: 459, Loss: 0.1664, Train: 0.4055, Val: 0.5236\n",
      "Epoch: 460, Loss: 0.1664, Train: 0.4054, Val: 0.5236\n",
      "Epoch: 461, Loss: 0.1663, Train: 0.4053, Val: 0.5237\n",
      "Epoch: 462, Loss: 0.1662, Train: 0.4052, Val: 0.5237\n",
      "Epoch: 463, Loss: 0.1662, Train: 0.4059, Val: 0.5238\n",
      "Epoch: 464, Loss: 0.1665, Train: 0.4053, Val: 0.5238\n",
      "Epoch: 465, Loss: 0.1664, Train: 0.4058, Val: 0.5239\n",
      "Epoch: 466, Loss: 0.1665, Train: 0.4047, Val: 0.5239\n",
      "Epoch: 467, Loss: 0.1658, Train: 0.4046, Val: 0.5240\n",
      "Epoch: 468, Loss: 0.1657, Train: 0.4054, Val: 0.5240\n",
      "Epoch: 469, Loss: 0.1662, Train: 0.4045, Val: 0.5241\n",
      "Epoch: 470, Loss: 0.1658, Train: 0.4046, Val: 0.5242\n",
      "Epoch: 471, Loss: 0.1656, Train: 0.4041, Val: 0.5243\n",
      "Epoch: 472, Loss: 0.1653, Train: 0.4040, Val: 0.5245\n",
      "Epoch: 473, Loss: 0.1653, Train: 0.4049, Val: 0.5246\n",
      "Epoch: 474, Loss: 0.1658, Train: 0.4041, Val: 0.5247\n",
      "Epoch: 475, Loss: 0.1656, Train: 0.4051, Val: 0.5249\n",
      "Epoch: 476, Loss: 0.1659, Train: 0.4035, Val: 0.5250\n",
      "Epoch: 477, Loss: 0.1650, Train: 0.4034, Val: 0.5251\n",
      "Epoch: 478, Loss: 0.1648, Train: 0.4032, Val: 0.5253\n",
      "Epoch: 479, Loss: 0.1647, Train: 0.4033, Val: 0.5254\n",
      "Epoch: 480, Loss: 0.1646, Train: 0.4032, Val: 0.5255\n",
      "Epoch: 481, Loss: 0.1647, Train: 0.4045, Val: 0.5256\n",
      "Epoch: 482, Loss: 0.1654, Train: 0.4031, Val: 0.5258\n",
      "Epoch: 483, Loss: 0.1647, Train: 0.4034, Val: 0.5260\n",
      "Epoch: 484, Loss: 0.1647, Train: 0.4025, Val: 0.5261\n",
      "Epoch: 485, Loss: 0.1642, Train: 0.4025, Val: 0.5262\n",
      "Epoch: 486, Loss: 0.1642, Train: 0.4037, Val: 0.5263\n",
      "Epoch: 487, Loss: 0.1648, Train: 0.4027, Val: 0.5265\n",
      "Epoch: 488, Loss: 0.1645, Train: 0.4034, Val: 0.5266\n",
      "Epoch: 489, Loss: 0.1646, Train: 0.4020, Val: 0.5267\n",
      "Epoch: 490, Loss: 0.1638, Train: 0.4018, Val: 0.5269\n",
      "Epoch: 491, Loss: 0.1636, Train: 0.4018, Val: 0.5270\n",
      "Epoch: 492, Loss: 0.1636, Train: 0.4017, Val: 0.5271\n",
      "Epoch: 493, Loss: 0.1636, Train: 0.4030, Val: 0.5272\n",
      "Epoch: 494, Loss: 0.1643, Train: 0.4017, Val: 0.5273\n",
      "Epoch: 495, Loss: 0.1638, Train: 0.4025, Val: 0.5274\n",
      "Epoch: 496, Loss: 0.1640, Train: 0.4011, Val: 0.5276\n",
      "Epoch: 497, Loss: 0.1632, Train: 0.4009, Val: 0.5277\n",
      "Epoch: 498, Loss: 0.1630, Train: 0.4008, Val: 0.5278\n",
      "Epoch: 499, Loss: 0.1628, Train: 0.4007, Val: 0.5279\n",
      "Epoch: 500, Loss: 0.1628, Train: 0.4017, Val: 0.5280\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train_data = train_data.to(device)\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "Test RMSE: 0.3173\n",
      "(3,)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_data = test_data.to(device)\n",
    "    pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
    "                 test_data['source', 'target'].edge_label_index)\n",
    "    print(pred.shape)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    target = test_data['source', 'target'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "sour = test_data['source', 'target'].edge_label_index[0].cpu().numpy()\n",
    "tar = test_data['source', 'target'].edge_label_index[1].cpu().numpy()\n",
    "pred = pred.cpu().numpy()\n",
    "print(pred.shape)\n",
    "target = target.cpu().numpy()\n",
    "\n",
    "res=pd.DataFrame({'source': sour, 'target': tar, 'pred': pred, 'compare': target})\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column if pred is greater or equal than 0.5 then 1 else 0.5\n",
    "res['weight'] = np.where(res['pred']>=0.5, 1., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "res\n",
    "print(res['weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Number of correct predictions: 3\n"
     ]
    }
   ],
   "source": [
    "#compare column rating_1 with target and if they are equal add up\n",
    "cont=0\n",
    "for i in res.itertuples():\n",
    "    if i.compare == i.weight:\n",
    "        cont+=1\n",
    "\n",
    "#Calculate the accuracy\n",
    "accuracy = cont/len(res)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Number of correct predictions:', cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = test_data\n",
    "\n",
    "pred_data['source','weight','target']['edge_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "pred_data['source','weight','target']['edge_label'] = torch.tensor(res['weight'], dtype=torch.long)\n",
    "print(pred_data['source','weight','target']['edge_label'])\n",
    "torch.save(pred_data, 'pred_data_150.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in connected edges:     2/2 = 1.0\n",
      "Accuracy in non connected edges: 1/1 = 1.0\n"
     ]
    }
   ],
   "source": [
    "connected_accuracy = 0.\n",
    "nonconnected_accuracy = 0.\n",
    "\n",
    "n1,n2=0,0\n",
    "ncon,nncon=0,0\n",
    "for i in res.itertuples():\n",
    "    if i.compare == 0.:\n",
    "        if i.compare == i.weight: n1+=1\n",
    "        nncon+=1\n",
    "    elif i.compare == 1.0:\n",
    "        if i.compare == i.weight: n2+=1\n",
    "        ncon+=1\n",
    "\n",
    "connected_accuracy = n2/ncon\n",
    "nonconnected_accuracy = n1/nncon\n",
    "\n",
    "print(f'Accuracy in connected edges:     {n2}/{ncon} = {connected_accuracy}')\n",
    "print(f'Accuracy in non connected edges: {n1}/{nncon} = {nonconnected_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_150_0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with different graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a new csv file\n",
    "file_test = cwd + '/points_10.csv'\n",
    "df_test=pd.read_csv(file_test, sep=',')\n",
    "\n",
    "#Round the values of the dataset to 4 decimal places\n",
    "df_test = df_test.round(4)\n",
    "\n",
    "#Add a column to use as index from 0 to the length of the dataset\n",
    "df_test['n_label'] = range(0, len(df_test))\n",
    "\n",
    "#delete the column p_label\n",
    "df_test = df_test.drop('p_label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10=HeteroData()\n",
    "\n",
    "nodes_s=df_test['n_label'].values\n",
    "nodes_t=df_test['n_label'].values\n",
    "\n",
    "data_10['source'].node_id = torch.tensor(nodes_s, dtype=torch.long)\n",
    "data_10['target'].node_id = torch.tensor(nodes_t, dtype=torch.long)\n",
    "\n",
    "data_10['source'].x = Tensor(df_test[['x', 'y', 'z']].values)\n",
    "data_10['target'].x = Tensor(df_test[['x', 'y', 'z']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_test_path = cwd + '/grap_10.csv'\n",
    "\n",
    "# Importing the dataset\n",
    "df_test_edge = pd.read_csv(edge_test_path)\n",
    "\n",
    "edge_index_test = torch.tensor([df_test_edge['Source'], df_test_edge['Target']], dtype=torch.long)\n",
    "\n",
    "data_10['source', 'weight', 'target'].edge_index = edge_index_test\n",
    "\n",
    "weight_test = torch.from_numpy(df_test_edge['weight'].values).to(torch.float)\n",
    "\n",
    "data_10['source', 'weight', 'target'].edge_label=weight_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10= T.ToUndirected()(data_10)\n",
    "del data_10['target', 'rev_weight', 'source'].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[100],\n",
       "    x=[100, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[100],\n",
       "    x=[100, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 107],\n",
       "    edge_label=[107],\n",
       "  },\n",
       "  (target, rev_weight, source)={ edge_index=[2, 107] }\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_10.validate(raise_on_error=True))\n",
    "data_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.4914\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    data_10 = data_10.to(device)\n",
    "    pred = model(data_10.x_dict, data_10.edge_index_dict,\n",
    "                 data_10['source', 'target'].edge_index)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    target = data_10['source', 'target'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "sour = data_10['source', 'target'].edge_index[0].cpu().numpy()\n",
    "tar = data_10['source', 'target'].edge_index[1].cpu().numpy()\n",
    "pred = pred.cpu().numpy()\n",
    "target = target.cpu().numpy()\n",
    "\n",
    "res=pd.DataFrame({'source': sour, 'target': tar, 'pred': pred, 'compare': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.514018691588785\n",
      "Number of correct predictions: 55\n"
     ]
    }
   ],
   "source": [
    "#Add a new column if pred is greater or equal than 0.5 then 1 else 0.5\n",
    "res['weight'] = np.where(res['pred']>0.5, 1, 0.5)\n",
    "\n",
    "#compare column rating_1 with target and if they are equal add up\n",
    "cont=0\n",
    "for i in res.itertuples():\n",
    "    if i.compare == i.weight:\n",
    "        cont+=1\n",
    "\n",
    "#Calculate the accuracy\n",
    "accuracy = cont/len(res)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Number of correct predictions:', cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
