{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import os\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "file_path = cwd + '/points_150.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>N_side</th>\n",
       "      <th>N_layer</th>\n",
       "      <th>t_label</th>\n",
       "      <th>phi</th>\n",
       "      <th>eta</th>\n",
       "      <th>q</th>\n",
       "      <th>pt</th>\n",
       "      <th>d0</th>\n",
       "      <th>z0</th>\n",
       "      <th>n_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8856</td>\n",
       "      <td>-5.8159</td>\n",
       "      <td>-29.5959</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.3150</td>\n",
       "      <td>-12.9382</td>\n",
       "      <td>-70.4852</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.7589</td>\n",
       "      <td>-20.0473</td>\n",
       "      <td>-111.3745</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.2174</td>\n",
       "      <td>-27.1433</td>\n",
       "      <td>-152.2638</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.6903</td>\n",
       "      <td>-34.2260</td>\n",
       "      <td>-193.1531</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>T0</td>\n",
       "      <td>-0.8395</td>\n",
       "      <td>-2.1563</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.6582</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x        y         z  N_side  N_layer t_label     phi     eta  q  \\\n",
       "0   7.8856  -5.8159  -29.5959      10        1      T0 -0.8395 -2.1563 -1   \n",
       "1  14.3150 -12.9382  -70.4852      10        2      T0 -0.8395 -2.1563 -1   \n",
       "2  20.7589 -20.0473 -111.3745      10        3      T0 -0.8395 -2.1563 -1   \n",
       "3  27.2174 -27.1433 -152.2638      10        4      T0 -0.8395 -2.1563 -1   \n",
       "4  33.6903 -34.2260 -193.1531      10        5      T0 -0.8395 -2.1563 -1   \n",
       "\n",
       "        pt      d0      z0  n_label  \n",
       "0  54.6582  0.0198  0.1129        0  \n",
       "1  54.6582  0.0198  0.1129        1  \n",
       "2  54.6582  0.0198  0.1129        2  \n",
       "3  54.6582  0.0198  0.1129        3  \n",
       "4  54.6582  0.0198  0.1129        4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(file_path)\n",
    "\n",
    "#Round the values of the dataset to 4 decimal places\n",
    "df = df.round(4)\n",
    "\n",
    "#Add a column to use as index from 0 to the length of the dataset\n",
    "df['n_label'] = range(0, len(df))\n",
    "\n",
    "#delete the column p_label\n",
    "df = df.drop('p_label', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty hetero graph \n",
    "data=HeteroData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node names\n",
    "nodes_s=df['n_label'].values\n",
    "nodes_t=df['n_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add nodes to the graph\n",
    "data['source'].node_id = torch.tensor(nodes_s, dtype=torch.long)\n",
    "data['target'].node_id = torch.tensor(nodes_t, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add node attributes, in this case the position of the points\n",
    "data['source'].x = Tensor(df[['x', 'y', 'z']].values)\n",
    "data['target'].x = Tensor(df[['x', 'y', 'z']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>471</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>1468</td>\n",
       "      <td>209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>1468</td>\n",
       "      <td>1469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>1478</td>\n",
       "      <td>1479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>1488</td>\n",
       "      <td>1489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>1498</td>\n",
       "      <td>1499</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3538 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Source  Target  weight\n",
       "0          0     111     1.0\n",
       "1          0     161     1.0\n",
       "2          0     181     1.0\n",
       "3          0     201     1.0\n",
       "4          0     471     1.0\n",
       "...      ...     ...     ...\n",
       "3533    1468     209     0.0\n",
       "3534    1468    1469     1.0\n",
       "3535    1478    1479     1.0\n",
       "3536    1488    1489     1.0\n",
       "3537    1498    1499     1.0\n",
       "\n",
       "[3538 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_path = cwd + '/grap_150.csv'\n",
    "\n",
    "# Importing the dataset\n",
    "df_edge = pd.read_csv(edge_path)\n",
    "df_edge = df_edge.replace({'weight':0.5}, 0.)\n",
    "df_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([df_edge['Source'], df_edge['Target']], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source', 'weight', 'target'].edge_index = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={ edge_index=[2, 3538] }\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge attributes\n",
    "weight_val = torch.from_numpy(df_edge['weight'].values).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source', 'weight', 'target'].edge_label=weight_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 3538],\n",
       "    edge_label=[3538],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if the data is valid\n",
    "data.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "data = T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['target', 'rev_weight', 'source'].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 3538],\n",
       "    edge_label=[3538],\n",
       "  },\n",
       "  (target, rev_weight, source)={ edge_index=[2, 3538] }\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.validate(raise_on_error=True))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('source', 'weight', 'target')],\n",
    "    rev_edge_types=[('target', 'rev_weight', 'source')],\n",
    ")(data)\n",
    "torch.save(test_data,'test_data_150.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[1500],\n",
       "    x=[1500, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 3185],\n",
       "    edge_label=[353],\n",
       "    edge_label_index=[2, 353],\n",
       "  },\n",
       "  (target, rev_weight, source)={ edge_index=[2, 3185] }\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (encoder): GraphModule(\n",
      "    (conv1): ModuleDict(\n",
      "      (source__weight__target): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "      (target__rev_weight__source): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "    )\n",
      "    (conv2): ModuleDict(\n",
      "      (source__weight__target): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "      (target__rev_weight__source): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "    )\n",
      "  )\n",
      "  (decoder): EdgeDecoder(\n",
      "    (lin1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (lin2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['source'][row], z_dict['target'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(hidden_channels=32).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['source', 'target'].edge_label_index)\n",
    "    target = train_data['source', 'target'].edge_label\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['source', 'target'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    target = data['source', 'target'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 18.8749, Train: 0.7026, Val: 0.6690\n",
      "Epoch: 002, Loss: 1575.4265, Train: 0.6979, Val: 0.6650\n",
      "Epoch: 003, Loss: 125.4004, Train: 0.7003, Val: 0.7035\n",
      "Epoch: 004, Loss: 38.0107, Train: 0.7098, Val: 0.7189\n",
      "Epoch: 005, Loss: 46.9315, Train: 0.6989, Val: 0.7022\n",
      "Epoch: 006, Loss: 7.9420, Train: 0.6464, Val: 0.6412\n",
      "Epoch: 007, Loss: 1.1411, Train: 0.6878, Val: 0.6664\n",
      "Epoch: 008, Loss: 5.5591, Train: 0.7099, Val: 0.6758\n",
      "Epoch: 009, Loss: 4.7799, Train: 0.6773, Val: 0.6640\n",
      "Epoch: 010, Loss: 3.2582, Train: 0.6734, Val: 0.6495\n",
      "Epoch: 011, Loss: 1.3877, Train: 0.6297, Val: 0.6283\n",
      "Epoch: 012, Loss: 0.8155, Train: 0.6129, Val: 0.6223\n",
      "Epoch: 013, Loss: 0.7442, Train: 0.6484, Val: 0.6116\n",
      "Epoch: 014, Loss: 1.2620, Train: 0.6508, Val: 0.6404\n",
      "Epoch: 015, Loss: 1.6627, Train: 0.6413, Val: 0.6234\n",
      "Epoch: 016, Loss: 0.8293, Train: 0.6536, Val: 0.6340\n",
      "Epoch: 017, Loss: 1.2798, Train: 0.6445, Val: 0.6503\n",
      "Epoch: 018, Loss: 1.3175, Train: 0.6289, Val: 0.6435\n",
      "Epoch: 019, Loss: 0.8630, Train: 0.6339, Val: 0.6115\n",
      "Epoch: 020, Loss: 1.2116, Train: 0.5974, Val: 0.6021\n",
      "Epoch: 021, Loss: 0.5770, Train: 0.5806, Val: 0.6003\n",
      "Epoch: 022, Loss: 0.7778, Train: 0.5770, Val: 0.5707\n",
      "Epoch: 023, Loss: 0.5625, Train: 0.5909, Val: 0.5806\n",
      "Epoch: 024, Loss: 0.5536, Train: 0.6028, Val: 0.6049\n",
      "Epoch: 025, Loss: 0.7214, Train: 0.5881, Val: 0.5985\n",
      "Epoch: 026, Loss: 0.4787, Train: 0.6077, Val: 0.5825\n",
      "Epoch: 027, Loss: 0.6133, Train: 0.5609, Val: 0.5727\n",
      "Epoch: 028, Loss: 0.4287, Train: 0.5386, Val: 0.5564\n",
      "Epoch: 029, Loss: 0.4018, Train: 0.5492, Val: 0.5436\n",
      "Epoch: 030, Loss: 0.4344, Train: 0.5445, Val: 0.5424\n",
      "Epoch: 031, Loss: 0.3508, Train: 0.5693, Val: 0.5600\n",
      "Epoch: 032, Loss: 0.4800, Train: 0.5583, Val: 0.5460\n",
      "Epoch: 033, Loss: 0.3807, Train: 0.5748, Val: 0.5488\n",
      "Epoch: 034, Loss: 0.4557, Train: 0.5510, Val: 0.5449\n",
      "Epoch: 035, Loss: 0.3682, Train: 0.5429, Val: 0.5394\n",
      "Epoch: 036, Loss: 0.3681, Train: 0.5306, Val: 0.5276\n",
      "Epoch: 037, Loss: 0.3376, Train: 0.5188, Val: 0.5237\n",
      "Epoch: 038, Loss: 0.3105, Train: 0.5253, Val: 0.5353\n",
      "Epoch: 039, Loss: 0.3414, Train: 0.5199, Val: 0.5298\n",
      "Epoch: 040, Loss: 0.3045, Train: 0.5427, Val: 0.5391\n",
      "Epoch: 041, Loss: 0.3472, Train: 0.5185, Val: 0.5285\n",
      "Epoch: 042, Loss: 0.2960, Train: 0.5275, Val: 0.5344\n",
      "Epoch: 043, Loss: 0.3141, Train: 0.5043, Val: 0.5186\n",
      "Epoch: 044, Loss: 0.2758, Train: 0.5091, Val: 0.5186\n",
      "Epoch: 045, Loss: 0.2840, Train: 0.5145, Val: 0.5206\n",
      "Epoch: 046, Loss: 0.2761, Train: 0.5190, Val: 0.5213\n",
      "Epoch: 047, Loss: 0.2798, Train: 0.5155, Val: 0.5188\n",
      "Epoch: 048, Loss: 0.2853, Train: 0.5110, Val: 0.5172\n",
      "Epoch: 049, Loss: 0.2758, Train: 0.5210, Val: 0.5216\n",
      "Epoch: 050, Loss: 0.2799, Train: 0.5036, Val: 0.5137\n",
      "Epoch: 051, Loss: 0.2615, Train: 0.5021, Val: 0.5151\n",
      "Epoch: 052, Loss: 0.2684, Train: 0.4945, Val: 0.5110\n",
      "Epoch: 053, Loss: 0.2536, Train: 0.5067, Val: 0.5176\n",
      "Epoch: 054, Loss: 0.2648, Train: 0.4953, Val: 0.5130\n",
      "Epoch: 055, Loss: 0.2559, Train: 0.4985, Val: 0.5168\n",
      "Epoch: 056, Loss: 0.2624, Train: 0.4967, Val: 0.5137\n",
      "Epoch: 057, Loss: 0.2547, Train: 0.4959, Val: 0.5127\n",
      "Epoch: 058, Loss: 0.2529, Train: 0.4898, Val: 0.5103\n",
      "Epoch: 059, Loss: 0.2496, Train: 0.4878, Val: 0.5078\n",
      "Epoch: 060, Loss: 0.2455, Train: 0.4938, Val: 0.5083\n",
      "Epoch: 061, Loss: 0.2485, Train: 0.4891, Val: 0.5059\n",
      "Epoch: 062, Loss: 0.2444, Train: 0.4907, Val: 0.5077\n",
      "Epoch: 063, Loss: 0.2484, Train: 0.4880, Val: 0.5053\n",
      "Epoch: 064, Loss: 0.2430, Train: 0.4898, Val: 0.5060\n",
      "Epoch: 065, Loss: 0.2441, Train: 0.4840, Val: 0.5046\n",
      "Epoch: 066, Loss: 0.2403, Train: 0.4828, Val: 0.5047\n",
      "Epoch: 067, Loss: 0.2396, Train: 0.4847, Val: 0.5054\n",
      "Epoch: 068, Loss: 0.2398, Train: 0.4821, Val: 0.5047\n",
      "Epoch: 069, Loss: 0.2380, Train: 0.4822, Val: 0.5055\n",
      "Epoch: 070, Loss: 0.2397, Train: 0.4810, Val: 0.5038\n",
      "Epoch: 071, Loss: 0.2368, Train: 0.4818, Val: 0.5031\n",
      "Epoch: 072, Loss: 0.2368, Train: 0.4791, Val: 0.5025\n",
      "Epoch: 073, Loss: 0.2352, Train: 0.4786, Val: 0.5012\n",
      "Epoch: 074, Loss: 0.2339, Train: 0.4807, Val: 0.5004\n",
      "Epoch: 075, Loss: 0.2346, Train: 0.4786, Val: 0.5000\n",
      "Epoch: 076, Loss: 0.2331, Train: 0.4787, Val: 0.5006\n",
      "Epoch: 077, Loss: 0.2337, Train: 0.4789, Val: 0.4993\n",
      "Epoch: 078, Loss: 0.2327, Train: 0.4776, Val: 0.4992\n",
      "Epoch: 079, Loss: 0.2316, Train: 0.4766, Val: 0.5006\n",
      "Epoch: 080, Loss: 0.2317, Train: 0.4762, Val: 0.4996\n",
      "Epoch: 081, Loss: 0.2305, Train: 0.4766, Val: 0.4999\n",
      "Epoch: 082, Loss: 0.2307, Train: 0.4754, Val: 0.5011\n",
      "Epoch: 083, Loss: 0.2305, Train: 0.4751, Val: 0.5003\n",
      "Epoch: 084, Loss: 0.2297, Train: 0.4758, Val: 0.4997\n",
      "Epoch: 085, Loss: 0.2297, Train: 0.4744, Val: 0.5001\n",
      "Epoch: 086, Loss: 0.2290, Train: 0.4742, Val: 0.4995\n",
      "Epoch: 087, Loss: 0.2284, Train: 0.4750, Val: 0.4986\n",
      "Epoch: 088, Loss: 0.2285, Train: 0.4740, Val: 0.4989\n",
      "Epoch: 089, Loss: 0.2279, Train: 0.4739, Val: 0.4988\n",
      "Epoch: 090, Loss: 0.2277, Train: 0.4742, Val: 0.4982\n",
      "Epoch: 091, Loss: 0.2276, Train: 0.4732, Val: 0.4985\n",
      "Epoch: 092, Loss: 0.2269, Train: 0.4728, Val: 0.4987\n",
      "Epoch: 093, Loss: 0.2267, Train: 0.4730, Val: 0.4983\n",
      "Epoch: 094, Loss: 0.2265, Train: 0.4722, Val: 0.4986\n",
      "Epoch: 095, Loss: 0.2261, Train: 0.4720, Val: 0.4987\n",
      "Epoch: 096, Loss: 0.2259, Train: 0.4722, Val: 0.4980\n",
      "Epoch: 097, Loss: 0.2257, Train: 0.4716, Val: 0.4980\n",
      "Epoch: 098, Loss: 0.2252, Train: 0.4714, Val: 0.4977\n",
      "Epoch: 099, Loss: 0.2250, Train: 0.4716, Val: 0.4970\n",
      "Epoch: 100, Loss: 0.2248, Train: 0.4710, Val: 0.4970\n",
      "Epoch: 101, Loss: 0.2244, Train: 0.4709, Val: 0.4969\n",
      "Epoch: 102, Loss: 0.2242, Train: 0.4709, Val: 0.4965\n",
      "Epoch: 103, Loss: 0.2240, Train: 0.4704, Val: 0.4968\n",
      "Epoch: 104, Loss: 0.2236, Train: 0.4701, Val: 0.4968\n",
      "Epoch: 105, Loss: 0.2233, Train: 0.4700, Val: 0.4967\n",
      "Epoch: 106, Loss: 0.2231, Train: 0.4696, Val: 0.4971\n",
      "Epoch: 107, Loss: 0.2228, Train: 0.4694, Val: 0.4970\n",
      "Epoch: 108, Loss: 0.2225, Train: 0.4692, Val: 0.4970\n",
      "Epoch: 109, Loss: 0.2223, Train: 0.4689, Val: 0.4971\n",
      "Epoch: 110, Loss: 0.2220, Train: 0.4688, Val: 0.4968\n",
      "Epoch: 111, Loss: 0.2217, Train: 0.4685, Val: 0.4967\n",
      "Epoch: 112, Loss: 0.2215, Train: 0.4682, Val: 0.4967\n",
      "Epoch: 113, Loss: 0.2213, Train: 0.4681, Val: 0.4966\n",
      "Epoch: 114, Loss: 0.2210, Train: 0.4677, Val: 0.4968\n",
      "Epoch: 115, Loss: 0.2208, Train: 0.4674, Val: 0.4969\n",
      "Epoch: 116, Loss: 0.2205, Train: 0.4672, Val: 0.4971\n",
      "Epoch: 117, Loss: 0.2202, Train: 0.4668, Val: 0.4975\n",
      "Epoch: 118, Loss: 0.2200, Train: 0.4667, Val: 0.4977\n",
      "Epoch: 119, Loss: 0.2198, Train: 0.4664, Val: 0.4979\n",
      "Epoch: 120, Loss: 0.2195, Train: 0.4661, Val: 0.4980\n",
      "Epoch: 121, Loss: 0.2193, Train: 0.4660, Val: 0.4979\n",
      "Epoch: 122, Loss: 0.2191, Train: 0.4657, Val: 0.4979\n",
      "Epoch: 123, Loss: 0.2189, Train: 0.4655, Val: 0.4978\n",
      "Epoch: 124, Loss: 0.2187, Train: 0.4654, Val: 0.4978\n",
      "Epoch: 125, Loss: 0.2185, Train: 0.4651, Val: 0.4977\n",
      "Epoch: 126, Loss: 0.2183, Train: 0.4649, Val: 0.4977\n",
      "Epoch: 127, Loss: 0.2180, Train: 0.4647, Val: 0.4977\n",
      "Epoch: 128, Loss: 0.2178, Train: 0.4644, Val: 0.4977\n",
      "Epoch: 129, Loss: 0.2176, Train: 0.4643, Val: 0.4976\n",
      "Epoch: 130, Loss: 0.2174, Train: 0.4640, Val: 0.4974\n",
      "Epoch: 131, Loss: 0.2172, Train: 0.4639, Val: 0.4973\n",
      "Epoch: 132, Loss: 0.2170, Train: 0.4637, Val: 0.4970\n",
      "Epoch: 133, Loss: 0.2167, Train: 0.4635, Val: 0.4968\n",
      "Epoch: 134, Loss: 0.2165, Train: 0.4633, Val: 0.4966\n",
      "Epoch: 135, Loss: 0.2163, Train: 0.4631, Val: 0.4964\n",
      "Epoch: 136, Loss: 0.2161, Train: 0.4630, Val: 0.4962\n",
      "Epoch: 137, Loss: 0.2159, Train: 0.4628, Val: 0.4961\n",
      "Epoch: 138, Loss: 0.2158, Train: 0.4626, Val: 0.4959\n",
      "Epoch: 139, Loss: 0.2156, Train: 0.4624, Val: 0.4959\n",
      "Epoch: 140, Loss: 0.2154, Train: 0.4622, Val: 0.4958\n",
      "Epoch: 141, Loss: 0.2152, Train: 0.4620, Val: 0.4958\n",
      "Epoch: 142, Loss: 0.2150, Train: 0.4618, Val: 0.4957\n",
      "Epoch: 143, Loss: 0.2148, Train: 0.4616, Val: 0.4957\n",
      "Epoch: 144, Loss: 0.2146, Train: 0.4614, Val: 0.4956\n",
      "Epoch: 145, Loss: 0.2144, Train: 0.4613, Val: 0.4956\n",
      "Epoch: 146, Loss: 0.2142, Train: 0.4610, Val: 0.4955\n",
      "Epoch: 147, Loss: 0.2141, Train: 0.4609, Val: 0.4956\n",
      "Epoch: 148, Loss: 0.2139, Train: 0.4606, Val: 0.4957\n",
      "Epoch: 149, Loss: 0.2137, Train: 0.4604, Val: 0.4958\n",
      "Epoch: 150, Loss: 0.2135, Train: 0.4601, Val: 0.4959\n",
      "Epoch: 151, Loss: 0.2133, Train: 0.4600, Val: 0.4960\n",
      "Epoch: 152, Loss: 0.2131, Train: 0.4597, Val: 0.4960\n",
      "Epoch: 153, Loss: 0.2130, Train: 0.4597, Val: 0.4962\n",
      "Epoch: 154, Loss: 0.2128, Train: 0.4594, Val: 0.4962\n",
      "Epoch: 155, Loss: 0.2127, Train: 0.4596, Val: 0.4964\n",
      "Epoch: 156, Loss: 0.2126, Train: 0.4593, Val: 0.4964\n",
      "Epoch: 157, Loss: 0.2127, Train: 0.4603, Val: 0.4970\n",
      "Epoch: 158, Loss: 0.2132, Train: 0.4610, Val: 0.4977\n",
      "Epoch: 159, Loss: 0.2146, Train: 0.4662, Val: 0.4997\n",
      "Epoch: 160, Loss: 0.2185, Train: 0.4732, Val: 0.5040\n",
      "Epoch: 161, Loss: 0.2286, Train: 0.4951, Val: 0.5096\n",
      "Epoch: 162, Loss: 0.2564, Train: 0.5204, Val: 0.5347\n",
      "Epoch: 163, Loss: 0.3242, Train: 0.5320, Val: 0.5160\n",
      "Epoch: 164, Loss: 0.4946, Train: 0.5690, Val: 0.5614\n",
      "Epoch: 165, Loss: 0.6896, Train: 0.5440, Val: 0.5203\n",
      "Epoch: 166, Loss: 0.7854, Train: 0.5275, Val: 0.5415\n",
      "Epoch: 167, Loss: 0.3382, Train: 0.5412, Val: 0.5467\n",
      "Epoch: 168, Loss: 0.3954, Train: 0.5443, Val: 0.5179\n",
      "Epoch: 169, Loss: 0.5360, Train: 0.4858, Val: 0.5049\n",
      "Epoch: 170, Loss: 0.2380, Train: 0.5538, Val: 0.5530\n",
      "Epoch: 171, Loss: 0.4742, Train: 0.5048, Val: 0.5016\n",
      "Epoch: 172, Loss: 0.2667, Train: 0.5270, Val: 0.5143\n",
      "Epoch: 173, Loss: 0.3639, Train: 0.5171, Val: 0.5294\n",
      "Epoch: 174, Loss: 0.2940, Train: 0.5210, Val: 0.5314\n",
      "Epoch: 175, Loss: 0.3006, Train: 0.5152, Val: 0.5117\n",
      "Epoch: 176, Loss: 0.2966, Train: 0.5081, Val: 0.5104\n",
      "Epoch: 177, Loss: 0.2741, Train: 0.5121, Val: 0.5272\n",
      "Epoch: 178, Loss: 0.2826, Train: 0.5022, Val: 0.5217\n",
      "Epoch: 179, Loss: 0.2649, Train: 0.5049, Val: 0.5130\n",
      "Epoch: 180, Loss: 0.2649, Train: 0.5055, Val: 0.5141\n",
      "Epoch: 181, Loss: 0.2651, Train: 0.4911, Val: 0.5132\n",
      "Epoch: 182, Loss: 0.2479, Train: 0.5029, Val: 0.5212\n",
      "Epoch: 183, Loss: 0.2666, Train: 0.4840, Val: 0.5061\n",
      "Epoch: 184, Loss: 0.2350, Train: 0.5054, Val: 0.5156\n",
      "Epoch: 185, Loss: 0.2651, Train: 0.4761, Val: 0.5026\n",
      "Epoch: 186, Loss: 0.2280, Train: 0.4994, Val: 0.5184\n",
      "Epoch: 187, Loss: 0.2593, Train: 0.4758, Val: 0.5022\n",
      "Epoch: 188, Loss: 0.2278, Train: 0.4960, Val: 0.5115\n",
      "Epoch: 189, Loss: 0.2495, Train: 0.4802, Val: 0.5035\n",
      "Epoch: 190, Loss: 0.2314, Train: 0.4846, Val: 0.5077\n",
      "Epoch: 191, Loss: 0.2384, Train: 0.4823, Val: 0.5060\n",
      "Epoch: 192, Loss: 0.2357, Train: 0.4781, Val: 0.5012\n",
      "Epoch: 193, Loss: 0.2293, Train: 0.4859, Val: 0.5054\n",
      "Epoch: 194, Loss: 0.2376, Train: 0.4728, Val: 0.4976\n",
      "Epoch: 195, Loss: 0.2246, Train: 0.4823, Val: 0.5045\n",
      "Epoch: 196, Loss: 0.2357, Train: 0.4720, Val: 0.4962\n",
      "Epoch: 197, Loss: 0.2239, Train: 0.4794, Val: 0.5001\n",
      "Epoch: 198, Loss: 0.2308, Train: 0.4741, Val: 0.4963\n",
      "Epoch: 199, Loss: 0.2253, Train: 0.4729, Val: 0.4956\n",
      "Epoch: 200, Loss: 0.2252, Train: 0.4741, Val: 0.4963\n",
      "Epoch: 201, Loss: 0.2266, Train: 0.4700, Val: 0.4925\n",
      "Epoch: 202, Loss: 0.2214, Train: 0.4749, Val: 0.4948\n",
      "Epoch: 203, Loss: 0.2262, Train: 0.4691, Val: 0.4915\n",
      "Epoch: 204, Loss: 0.2206, Train: 0.4716, Val: 0.4933\n",
      "Epoch: 205, Loss: 0.2238, Train: 0.4698, Val: 0.4918\n",
      "Epoch: 206, Loss: 0.2218, Train: 0.4696, Val: 0.4910\n",
      "Epoch: 207, Loss: 0.2210, Train: 0.4712, Val: 0.4918\n",
      "Epoch: 208, Loss: 0.2224, Train: 0.4675, Val: 0.4902\n",
      "Epoch: 209, Loss: 0.2193, Train: 0.4690, Val: 0.4920\n",
      "Epoch: 210, Loss: 0.2213, Train: 0.4667, Val: 0.4906\n",
      "Epoch: 211, Loss: 0.2187, Train: 0.4677, Val: 0.4912\n",
      "Epoch: 212, Loss: 0.2192, Train: 0.4670, Val: 0.4913\n",
      "Epoch: 213, Loss: 0.2186, Train: 0.4654, Val: 0.4915\n",
      "Epoch: 214, Loss: 0.2176, Train: 0.4659, Val: 0.4927\n",
      "Epoch: 215, Loss: 0.2184, Train: 0.4646, Val: 0.4913\n",
      "Epoch: 216, Loss: 0.2168, Train: 0.4659, Val: 0.4919\n",
      "Epoch: 217, Loss: 0.2178, Train: 0.4639, Val: 0.4914\n",
      "Epoch: 218, Loss: 0.2161, Train: 0.4641, Val: 0.4925\n",
      "Epoch: 219, Loss: 0.2167, Train: 0.4631, Val: 0.4916\n",
      "Epoch: 220, Loss: 0.2156, Train: 0.4635, Val: 0.4913\n",
      "Epoch: 221, Loss: 0.2157, Train: 0.4629, Val: 0.4911\n",
      "Epoch: 222, Loss: 0.2151, Train: 0.4621, Val: 0.4914\n",
      "Epoch: 223, Loss: 0.2147, Train: 0.4620, Val: 0.4916\n",
      "Epoch: 224, Loss: 0.2146, Train: 0.4617, Val: 0.4910\n",
      "Epoch: 225, Loss: 0.2140, Train: 0.4619, Val: 0.4912\n",
      "Epoch: 226, Loss: 0.2141, Train: 0.4608, Val: 0.4911\n",
      "Epoch: 227, Loss: 0.2134, Train: 0.4607, Val: 0.4915\n",
      "Epoch: 228, Loss: 0.2135, Train: 0.4602, Val: 0.4909\n",
      "Epoch: 229, Loss: 0.2127, Train: 0.4604, Val: 0.4907\n",
      "Epoch: 230, Loss: 0.2128, Train: 0.4596, Val: 0.4905\n",
      "Epoch: 231, Loss: 0.2122, Train: 0.4593, Val: 0.4909\n",
      "Epoch: 232, Loss: 0.2122, Train: 0.4589, Val: 0.4905\n",
      "Epoch: 233, Loss: 0.2117, Train: 0.4591, Val: 0.4904\n",
      "Epoch: 234, Loss: 0.2117, Train: 0.4585, Val: 0.4904\n",
      "Epoch: 235, Loss: 0.2113, Train: 0.4582, Val: 0.4907\n",
      "Epoch: 236, Loss: 0.2111, Train: 0.4578, Val: 0.4906\n",
      "Epoch: 237, Loss: 0.2108, Train: 0.4579, Val: 0.4905\n",
      "Epoch: 238, Loss: 0.2106, Train: 0.4574, Val: 0.4906\n",
      "Epoch: 239, Loss: 0.2103, Train: 0.4571, Val: 0.4908\n",
      "Epoch: 240, Loss: 0.2101, Train: 0.4568, Val: 0.4907\n",
      "Epoch: 241, Loss: 0.2098, Train: 0.4568, Val: 0.4907\n",
      "Epoch: 242, Loss: 0.2096, Train: 0.4565, Val: 0.4907\n",
      "Epoch: 243, Loss: 0.2094, Train: 0.4562, Val: 0.4908\n",
      "Epoch: 244, Loss: 0.2092, Train: 0.4559, Val: 0.4907\n",
      "Epoch: 245, Loss: 0.2089, Train: 0.4558, Val: 0.4906\n",
      "Epoch: 246, Loss: 0.2088, Train: 0.4555, Val: 0.4905\n",
      "Epoch: 247, Loss: 0.2085, Train: 0.4551, Val: 0.4906\n",
      "Epoch: 248, Loss: 0.2083, Train: 0.4549, Val: 0.4905\n",
      "Epoch: 249, Loss: 0.2080, Train: 0.4548, Val: 0.4905\n",
      "Epoch: 250, Loss: 0.2078, Train: 0.4544, Val: 0.4905\n",
      "Epoch: 251, Loss: 0.2076, Train: 0.4541, Val: 0.4905\n",
      "Epoch: 252, Loss: 0.2074, Train: 0.4539, Val: 0.4904\n",
      "Epoch: 253, Loss: 0.2071, Train: 0.4537, Val: 0.4904\n",
      "Epoch: 254, Loss: 0.2069, Train: 0.4534, Val: 0.4903\n",
      "Epoch: 255, Loss: 0.2066, Train: 0.4531, Val: 0.4903\n",
      "Epoch: 256, Loss: 0.2064, Train: 0.4529, Val: 0.4901\n",
      "Epoch: 257, Loss: 0.2062, Train: 0.4527, Val: 0.4901\n",
      "Epoch: 258, Loss: 0.2060, Train: 0.4524, Val: 0.4900\n",
      "Epoch: 259, Loss: 0.2057, Train: 0.4522, Val: 0.4899\n",
      "Epoch: 260, Loss: 0.2055, Train: 0.4519, Val: 0.4898\n",
      "Epoch: 261, Loss: 0.2053, Train: 0.4517, Val: 0.4898\n",
      "Epoch: 262, Loss: 0.2051, Train: 0.4514, Val: 0.4897\n",
      "Epoch: 263, Loss: 0.2048, Train: 0.4511, Val: 0.4895\n",
      "Epoch: 264, Loss: 0.2046, Train: 0.4509, Val: 0.4894\n",
      "Epoch: 265, Loss: 0.2044, Train: 0.4506, Val: 0.4893\n",
      "Epoch: 266, Loss: 0.2041, Train: 0.4504, Val: 0.4893\n",
      "Epoch: 267, Loss: 0.2039, Train: 0.4501, Val: 0.4891\n",
      "Epoch: 268, Loss: 0.2036, Train: 0.4499, Val: 0.4890\n",
      "Epoch: 269, Loss: 0.2034, Train: 0.4496, Val: 0.4889\n",
      "Epoch: 270, Loss: 0.2031, Train: 0.4493, Val: 0.4887\n",
      "Epoch: 271, Loss: 0.2029, Train: 0.4491, Val: 0.4886\n",
      "Epoch: 272, Loss: 0.2027, Train: 0.4488, Val: 0.4884\n",
      "Epoch: 273, Loss: 0.2024, Train: 0.4486, Val: 0.4883\n",
      "Epoch: 274, Loss: 0.2022, Train: 0.4483, Val: 0.4882\n",
      "Epoch: 275, Loss: 0.2020, Train: 0.4481, Val: 0.4880\n",
      "Epoch: 276, Loss: 0.2017, Train: 0.4478, Val: 0.4879\n",
      "Epoch: 277, Loss: 0.2015, Train: 0.4476, Val: 0.4878\n",
      "Epoch: 278, Loss: 0.2013, Train: 0.4473, Val: 0.4877\n",
      "Epoch: 279, Loss: 0.2010, Train: 0.4471, Val: 0.4876\n",
      "Epoch: 280, Loss: 0.2008, Train: 0.4469, Val: 0.4876\n",
      "Epoch: 281, Loss: 0.2006, Train: 0.4466, Val: 0.4876\n",
      "Epoch: 282, Loss: 0.2004, Train: 0.4464, Val: 0.4876\n",
      "Epoch: 283, Loss: 0.2002, Train: 0.4462, Val: 0.4876\n",
      "Epoch: 284, Loss: 0.2000, Train: 0.4460, Val: 0.4876\n",
      "Epoch: 285, Loss: 0.1998, Train: 0.4458, Val: 0.4876\n",
      "Epoch: 286, Loss: 0.1997, Train: 0.4456, Val: 0.4876\n",
      "Epoch: 287, Loss: 0.1995, Train: 0.4454, Val: 0.4876\n",
      "Epoch: 288, Loss: 0.1993, Train: 0.4452, Val: 0.4876\n",
      "Epoch: 289, Loss: 0.1991, Train: 0.4450, Val: 0.4875\n",
      "Epoch: 290, Loss: 0.1989, Train: 0.4447, Val: 0.4875\n",
      "Epoch: 291, Loss: 0.1987, Train: 0.4445, Val: 0.4875\n",
      "Epoch: 292, Loss: 0.1986, Train: 0.4444, Val: 0.4874\n",
      "Epoch: 293, Loss: 0.1984, Train: 0.4442, Val: 0.4874\n",
      "Epoch: 294, Loss: 0.1982, Train: 0.4439, Val: 0.4874\n",
      "Epoch: 295, Loss: 0.1981, Train: 0.4438, Val: 0.4874\n",
      "Epoch: 296, Loss: 0.1979, Train: 0.4436, Val: 0.4873\n",
      "Epoch: 297, Loss: 0.1977, Train: 0.4434, Val: 0.4873\n",
      "Epoch: 298, Loss: 0.1976, Train: 0.4432, Val: 0.4873\n",
      "Epoch: 299, Loss: 0.1974, Train: 0.4430, Val: 0.4874\n",
      "Epoch: 300, Loss: 0.1972, Train: 0.4428, Val: 0.4874\n",
      "Epoch: 301, Loss: 0.1971, Train: 0.4426, Val: 0.4875\n",
      "Epoch: 302, Loss: 0.1969, Train: 0.4424, Val: 0.4875\n",
      "Epoch: 303, Loss: 0.1967, Train: 0.4422, Val: 0.4875\n",
      "Epoch: 304, Loss: 0.1966, Train: 0.4420, Val: 0.4875\n",
      "Epoch: 305, Loss: 0.1964, Train: 0.4418, Val: 0.4875\n",
      "Epoch: 306, Loss: 0.1963, Train: 0.4416, Val: 0.4874\n",
      "Epoch: 307, Loss: 0.1961, Train: 0.4414, Val: 0.4874\n",
      "Epoch: 308, Loss: 0.1959, Train: 0.4413, Val: 0.4874\n",
      "Epoch: 309, Loss: 0.1958, Train: 0.4411, Val: 0.4874\n",
      "Epoch: 310, Loss: 0.1956, Train: 0.4409, Val: 0.4873\n",
      "Epoch: 311, Loss: 0.1955, Train: 0.4407, Val: 0.4873\n",
      "Epoch: 312, Loss: 0.1953, Train: 0.4405, Val: 0.4873\n",
      "Epoch: 313, Loss: 0.1951, Train: 0.4403, Val: 0.4873\n",
      "Epoch: 314, Loss: 0.1950, Train: 0.4401, Val: 0.4872\n",
      "Epoch: 315, Loss: 0.1948, Train: 0.4399, Val: 0.4872\n",
      "Epoch: 316, Loss: 0.1947, Train: 0.4397, Val: 0.4872\n",
      "Epoch: 317, Loss: 0.1945, Train: 0.4396, Val: 0.4871\n",
      "Epoch: 318, Loss: 0.1943, Train: 0.4394, Val: 0.4871\n",
      "Epoch: 319, Loss: 0.1942, Train: 0.4392, Val: 0.4871\n",
      "Epoch: 320, Loss: 0.1940, Train: 0.4390, Val: 0.4871\n",
      "Epoch: 321, Loss: 0.1939, Train: 0.4388, Val: 0.4871\n",
      "Epoch: 322, Loss: 0.1937, Train: 0.4386, Val: 0.4871\n",
      "Epoch: 323, Loss: 0.1936, Train: 0.4385, Val: 0.4871\n",
      "Epoch: 324, Loss: 0.1934, Train: 0.4383, Val: 0.4871\n",
      "Epoch: 325, Loss: 0.1933, Train: 0.4381, Val: 0.4871\n",
      "Epoch: 326, Loss: 0.1931, Train: 0.4379, Val: 0.4871\n",
      "Epoch: 327, Loss: 0.1929, Train: 0.4377, Val: 0.4872\n",
      "Epoch: 328, Loss: 0.1928, Train: 0.4376, Val: 0.4871\n",
      "Epoch: 329, Loss: 0.1927, Train: 0.4374, Val: 0.4871\n",
      "Epoch: 330, Loss: 0.1925, Train: 0.4372, Val: 0.4870\n",
      "Epoch: 331, Loss: 0.1924, Train: 0.4371, Val: 0.4870\n",
      "Epoch: 332, Loss: 0.1922, Train: 0.4369, Val: 0.4870\n",
      "Epoch: 333, Loss: 0.1921, Train: 0.4367, Val: 0.4870\n",
      "Epoch: 334, Loss: 0.1919, Train: 0.4366, Val: 0.4869\n",
      "Epoch: 335, Loss: 0.1918, Train: 0.4364, Val: 0.4869\n",
      "Epoch: 336, Loss: 0.1916, Train: 0.4362, Val: 0.4869\n",
      "Epoch: 337, Loss: 0.1915, Train: 0.4361, Val: 0.4869\n",
      "Epoch: 338, Loss: 0.1914, Train: 0.4359, Val: 0.4869\n",
      "Epoch: 339, Loss: 0.1912, Train: 0.4358, Val: 0.4870\n",
      "Epoch: 340, Loss: 0.1911, Train: 0.4356, Val: 0.4870\n",
      "Epoch: 341, Loss: 0.1910, Train: 0.4355, Val: 0.4870\n",
      "Epoch: 342, Loss: 0.1908, Train: 0.4353, Val: 0.4870\n",
      "Epoch: 343, Loss: 0.1907, Train: 0.4351, Val: 0.4869\n",
      "Epoch: 344, Loss: 0.1906, Train: 0.4350, Val: 0.4869\n",
      "Epoch: 345, Loss: 0.1904, Train: 0.4348, Val: 0.4869\n",
      "Epoch: 346, Loss: 0.1903, Train: 0.4347, Val: 0.4869\n",
      "Epoch: 347, Loss: 0.1902, Train: 0.4345, Val: 0.4869\n",
      "Epoch: 348, Loss: 0.1900, Train: 0.4344, Val: 0.4869\n",
      "Epoch: 349, Loss: 0.1899, Train: 0.4342, Val: 0.4869\n",
      "Epoch: 350, Loss: 0.1898, Train: 0.4341, Val: 0.4869\n",
      "Epoch: 351, Loss: 0.1896, Train: 0.4339, Val: 0.4869\n",
      "Epoch: 352, Loss: 0.1895, Train: 0.4338, Val: 0.4870\n",
      "Epoch: 353, Loss: 0.1894, Train: 0.4336, Val: 0.4870\n",
      "Epoch: 354, Loss: 0.1892, Train: 0.4335, Val: 0.4870\n",
      "Epoch: 355, Loss: 0.1891, Train: 0.4333, Val: 0.4870\n",
      "Epoch: 356, Loss: 0.1890, Train: 0.4332, Val: 0.4871\n",
      "Epoch: 357, Loss: 0.1889, Train: 0.4330, Val: 0.4871\n",
      "Epoch: 358, Loss: 0.1887, Train: 0.4329, Val: 0.4871\n",
      "Epoch: 359, Loss: 0.1886, Train: 0.4327, Val: 0.4871\n",
      "Epoch: 360, Loss: 0.1885, Train: 0.4326, Val: 0.4871\n",
      "Epoch: 361, Loss: 0.1884, Train: 0.4324, Val: 0.4871\n",
      "Epoch: 362, Loss: 0.1882, Train: 0.4323, Val: 0.4871\n",
      "Epoch: 363, Loss: 0.1881, Train: 0.4321, Val: 0.4872\n",
      "Epoch: 364, Loss: 0.1880, Train: 0.4320, Val: 0.4872\n",
      "Epoch: 365, Loss: 0.1879, Train: 0.4319, Val: 0.4872\n",
      "Epoch: 366, Loss: 0.1878, Train: 0.4317, Val: 0.4873\n",
      "Epoch: 367, Loss: 0.1876, Train: 0.4316, Val: 0.4873\n",
      "Epoch: 368, Loss: 0.1875, Train: 0.4314, Val: 0.4874\n",
      "Epoch: 369, Loss: 0.1874, Train: 0.4313, Val: 0.4874\n",
      "Epoch: 370, Loss: 0.1873, Train: 0.4311, Val: 0.4874\n",
      "Epoch: 371, Loss: 0.1872, Train: 0.4310, Val: 0.4874\n",
      "Epoch: 372, Loss: 0.1870, Train: 0.4308, Val: 0.4874\n",
      "Epoch: 373, Loss: 0.1869, Train: 0.4307, Val: 0.4875\n",
      "Epoch: 374, Loss: 0.1868, Train: 0.4306, Val: 0.4875\n",
      "Epoch: 375, Loss: 0.1867, Train: 0.4304, Val: 0.4875\n",
      "Epoch: 376, Loss: 0.1866, Train: 0.4303, Val: 0.4876\n",
      "Epoch: 377, Loss: 0.1864, Train: 0.4301, Val: 0.4876\n",
      "Epoch: 378, Loss: 0.1863, Train: 0.4300, Val: 0.4876\n",
      "Epoch: 379, Loss: 0.1862, Train: 0.4299, Val: 0.4877\n",
      "Epoch: 380, Loss: 0.1861, Train: 0.4297, Val: 0.4877\n",
      "Epoch: 381, Loss: 0.1860, Train: 0.4296, Val: 0.4878\n",
      "Epoch: 382, Loss: 0.1858, Train: 0.4294, Val: 0.4879\n",
      "Epoch: 383, Loss: 0.1857, Train: 0.4293, Val: 0.4879\n",
      "Epoch: 384, Loss: 0.1856, Train: 0.4292, Val: 0.4880\n",
      "Epoch: 385, Loss: 0.1855, Train: 0.4290, Val: 0.4881\n",
      "Epoch: 386, Loss: 0.1854, Train: 0.4289, Val: 0.4882\n",
      "Epoch: 387, Loss: 0.1853, Train: 0.4287, Val: 0.4882\n",
      "Epoch: 388, Loss: 0.1851, Train: 0.4286, Val: 0.4883\n",
      "Epoch: 389, Loss: 0.1850, Train: 0.4284, Val: 0.4884\n",
      "Epoch: 390, Loss: 0.1849, Train: 0.4283, Val: 0.4884\n",
      "Epoch: 391, Loss: 0.1848, Train: 0.4282, Val: 0.4885\n",
      "Epoch: 392, Loss: 0.1847, Train: 0.4280, Val: 0.4885\n",
      "Epoch: 393, Loss: 0.1846, Train: 0.4279, Val: 0.4886\n",
      "Epoch: 394, Loss: 0.1845, Train: 0.4277, Val: 0.4886\n",
      "Epoch: 395, Loss: 0.1843, Train: 0.4276, Val: 0.4886\n",
      "Epoch: 396, Loss: 0.1842, Train: 0.4275, Val: 0.4887\n",
      "Epoch: 397, Loss: 0.1841, Train: 0.4273, Val: 0.4887\n",
      "Epoch: 398, Loss: 0.1840, Train: 0.4272, Val: 0.4888\n",
      "Epoch: 399, Loss: 0.1839, Train: 0.4271, Val: 0.4889\n",
      "Epoch: 400, Loss: 0.1838, Train: 0.4269, Val: 0.4889\n",
      "Epoch: 401, Loss: 0.1837, Train: 0.4268, Val: 0.4890\n",
      "Epoch: 402, Loss: 0.1835, Train: 0.4267, Val: 0.4890\n",
      "Epoch: 403, Loss: 0.1834, Train: 0.4265, Val: 0.4890\n",
      "Epoch: 404, Loss: 0.1833, Train: 0.4264, Val: 0.4891\n",
      "Epoch: 405, Loss: 0.1832, Train: 0.4262, Val: 0.4892\n",
      "Epoch: 406, Loss: 0.1831, Train: 0.4261, Val: 0.4893\n",
      "Epoch: 407, Loss: 0.1830, Train: 0.4260, Val: 0.4893\n",
      "Epoch: 408, Loss: 0.1829, Train: 0.4258, Val: 0.4895\n",
      "Epoch: 409, Loss: 0.1827, Train: 0.4257, Val: 0.4895\n",
      "Epoch: 410, Loss: 0.1826, Train: 0.4256, Val: 0.4896\n",
      "Epoch: 411, Loss: 0.1825, Train: 0.4254, Val: 0.4896\n",
      "Epoch: 412, Loss: 0.1824, Train: 0.4253, Val: 0.4897\n",
      "Epoch: 413, Loss: 0.1823, Train: 0.4252, Val: 0.4897\n",
      "Epoch: 414, Loss: 0.1822, Train: 0.4251, Val: 0.4898\n",
      "Epoch: 415, Loss: 0.1821, Train: 0.4249, Val: 0.4897\n",
      "Epoch: 416, Loss: 0.1820, Train: 0.4249, Val: 0.4900\n",
      "Epoch: 417, Loss: 0.1819, Train: 0.4247, Val: 0.4898\n",
      "Epoch: 418, Loss: 0.1819, Train: 0.4248, Val: 0.4904\n",
      "Epoch: 419, Loss: 0.1819, Train: 0.4248, Val: 0.4900\n",
      "Epoch: 420, Loss: 0.1819, Train: 0.4252, Val: 0.4908\n",
      "Epoch: 421, Loss: 0.1822, Train: 0.4259, Val: 0.4905\n",
      "Epoch: 422, Loss: 0.1830, Train: 0.4282, Val: 0.4924\n",
      "Epoch: 423, Loss: 0.1848, Train: 0.4323, Val: 0.4936\n",
      "Epoch: 424, Loss: 0.1889, Train: 0.4429, Val: 0.5000\n",
      "Epoch: 425, Loss: 0.1980, Train: 0.4593, Val: 0.5089\n",
      "Epoch: 426, Loss: 0.2185, Train: 0.4849, Val: 0.5155\n",
      "Epoch: 427, Loss: 0.2591, Train: 0.5136, Val: 0.5400\n",
      "Epoch: 428, Loss: 0.3375, Train: 0.5145, Val: 0.5298\n",
      "Epoch: 429, Loss: 0.4325, Train: 0.5371, Val: 0.5536\n",
      "Epoch: 430, Loss: 0.4664, Train: 0.5070, Val: 0.5241\n",
      "Epoch: 431, Loss: 0.3455, Train: 0.4404, Val: 0.4995\n",
      "Epoch: 432, Loss: 0.1961, Train: 0.4743, Val: 0.5197\n",
      "Epoch: 433, Loss: 0.2394, Train: 0.4986, Val: 0.5223\n",
      "Epoch: 434, Loss: 0.3059, Train: 0.4554, Val: 0.5099\n",
      "Epoch: 435, Loss: 0.2124, Train: 0.4479, Val: 0.5059\n",
      "Epoch: 436, Loss: 0.2037, Train: 0.4895, Val: 0.5244\n",
      "Epoch: 437, Loss: 0.2647, Train: 0.4490, Val: 0.5077\n",
      "Epoch: 438, Loss: 0.2048, Train: 0.4459, Val: 0.5065\n",
      "Epoch: 439, Loss: 0.2014, Train: 0.4763, Val: 0.5186\n",
      "Epoch: 440, Loss: 0.2344, Train: 0.4311, Val: 0.4988\n",
      "Epoch: 441, Loss: 0.1867, Train: 0.4571, Val: 0.5150\n",
      "Epoch: 442, Loss: 0.2141, Train: 0.4474, Val: 0.5045\n",
      "Epoch: 443, Loss: 0.2010, Train: 0.4393, Val: 0.5015\n",
      "Epoch: 444, Loss: 0.1936, Train: 0.4518, Val: 0.5124\n",
      "Epoch: 445, Loss: 0.2080, Train: 0.4293, Val: 0.4994\n",
      "Epoch: 446, Loss: 0.1849, Train: 0.4528, Val: 0.5125\n",
      "Epoch: 447, Loss: 0.2063, Train: 0.4339, Val: 0.5016\n",
      "Epoch: 448, Loss: 0.1894, Train: 0.4386, Val: 0.5037\n",
      "Epoch: 449, Loss: 0.1939, Train: 0.4404, Val: 0.5041\n",
      "Epoch: 450, Loss: 0.1947, Train: 0.4308, Val: 0.4987\n",
      "Epoch: 451, Loss: 0.1862, Train: 0.4402, Val: 0.5045\n",
      "Epoch: 452, Loss: 0.1956, Train: 0.4280, Val: 0.4973\n",
      "Epoch: 453, Loss: 0.1838, Train: 0.4388, Val: 0.5032\n",
      "Epoch: 454, Loss: 0.1932, Train: 0.4290, Val: 0.4978\n",
      "Epoch: 455, Loss: 0.1849, Train: 0.4323, Val: 0.4993\n",
      "Epoch: 456, Loss: 0.1881, Train: 0.4320, Val: 0.4993\n",
      "Epoch: 457, Loss: 0.1873, Train: 0.4279, Val: 0.4968\n",
      "Epoch: 458, Loss: 0.1838, Train: 0.4323, Val: 0.4987\n",
      "Epoch: 459, Loss: 0.1883, Train: 0.4259, Val: 0.4957\n",
      "Epoch: 460, Loss: 0.1822, Train: 0.4313, Val: 0.4987\n",
      "Epoch: 461, Loss: 0.1867, Train: 0.4270, Val: 0.4960\n",
      "Epoch: 462, Loss: 0.1834, Train: 0.4272, Val: 0.4959\n",
      "Epoch: 463, Loss: 0.1837, Train: 0.4289, Val: 0.4970\n",
      "Epoch: 464, Loss: 0.1847, Train: 0.4250, Val: 0.4946\n",
      "Epoch: 465, Loss: 0.1816, Train: 0.4277, Val: 0.4958\n",
      "Epoch: 466, Loss: 0.1843, Train: 0.4250, Val: 0.4949\n",
      "Epoch: 467, Loss: 0.1816, Train: 0.4261, Val: 0.4957\n",
      "Epoch: 468, Loss: 0.1825, Train: 0.4257, Val: 0.4952\n",
      "Epoch: 469, Loss: 0.1826, Train: 0.4238, Val: 0.4946\n",
      "Epoch: 470, Loss: 0.1807, Train: 0.4260, Val: 0.4959\n",
      "Epoch: 471, Loss: 0.1824, Train: 0.4237, Val: 0.4944\n",
      "Epoch: 472, Loss: 0.1807, Train: 0.4238, Val: 0.4943\n",
      "Epoch: 473, Loss: 0.1809, Train: 0.4245, Val: 0.4949\n",
      "Epoch: 474, Loss: 0.1812, Train: 0.4227, Val: 0.4940\n",
      "Epoch: 475, Loss: 0.1798, Train: 0.4235, Val: 0.4944\n",
      "Epoch: 476, Loss: 0.1807, Train: 0.4229, Val: 0.4946\n",
      "Epoch: 477, Loss: 0.1799, Train: 0.4224, Val: 0.4944\n",
      "Epoch: 478, Loss: 0.1795, Train: 0.4227, Val: 0.4943\n",
      "Epoch: 479, Loss: 0.1800, Train: 0.4218, Val: 0.4941\n",
      "Epoch: 480, Loss: 0.1791, Train: 0.4220, Val: 0.4942\n",
      "Epoch: 481, Loss: 0.1792, Train: 0.4218, Val: 0.4940\n",
      "Epoch: 482, Loss: 0.1793, Train: 0.4211, Val: 0.4939\n",
      "Epoch: 483, Loss: 0.1785, Train: 0.4215, Val: 0.4943\n",
      "Epoch: 484, Loss: 0.1789, Train: 0.4211, Val: 0.4940\n",
      "Epoch: 485, Loss: 0.1786, Train: 0.4205, Val: 0.4939\n",
      "Epoch: 486, Loss: 0.1781, Train: 0.4209, Val: 0.4944\n",
      "Epoch: 487, Loss: 0.1784, Train: 0.4204, Val: 0.4940\n",
      "Epoch: 488, Loss: 0.1781, Train: 0.4200, Val: 0.4939\n",
      "Epoch: 489, Loss: 0.1777, Train: 0.4203, Val: 0.4942\n",
      "Epoch: 490, Loss: 0.1779, Train: 0.4198, Val: 0.4939\n",
      "Epoch: 491, Loss: 0.1776, Train: 0.4195, Val: 0.4939\n",
      "Epoch: 492, Loss: 0.1773, Train: 0.4197, Val: 0.4942\n",
      "Epoch: 493, Loss: 0.1774, Train: 0.4193, Val: 0.4940\n",
      "Epoch: 494, Loss: 0.1772, Train: 0.4190, Val: 0.4939\n",
      "Epoch: 495, Loss: 0.1769, Train: 0.4191, Val: 0.4942\n",
      "Epoch: 496, Loss: 0.1770, Train: 0.4188, Val: 0.4940\n",
      "Epoch: 497, Loss: 0.1768, Train: 0.4185, Val: 0.4939\n",
      "Epoch: 498, Loss: 0.1765, Train: 0.4186, Val: 0.4942\n",
      "Epoch: 499, Loss: 0.1765, Train: 0.4183, Val: 0.4941\n",
      "Epoch: 500, Loss: 0.1764, Train: 0.4181, Val: 0.4942\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train_data = train_data.to(device)\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([353])\n",
      "Test RMSE: 0.4673\n",
      "(353,)\n",
      "(353, 4)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_data = test_data.to(device)\n",
    "    pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
    "                 test_data['source', 'target'].edge_label_index)\n",
    "    print(pred.shape)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    target = test_data['source', 'target'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "sour = test_data['source', 'target'].edge_label_index[0].cpu().numpy()\n",
    "tar = test_data['source', 'target'].edge_label_index[1].cpu().numpy()\n",
    "pred = pred.cpu().numpy()\n",
    "print(pred.shape)\n",
    "target = target.cpu().numpy()\n",
    "\n",
    "res=pd.DataFrame({'source': sour, 'target': tar, 'pred': pred, 'compare': target})\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column if pred is greater or equal than 0.5 then 1 else 0.5\n",
    "res['weight'] = np.where(res['pred']>=0.5, 1., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353,)\n"
     ]
    }
   ],
   "source": [
    "res\n",
    "print(res['weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6628895184135978\n",
      "Number of correct predictions: 234\n"
     ]
    }
   ],
   "source": [
    "#compare column rating_1 with target and if they are equal add up\n",
    "cont=0\n",
    "for i in res.itertuples():\n",
    "    if i.compare == i.weight:\n",
    "        cont+=1\n",
    "\n",
    "#Calculate the accuracy\n",
    "accuracy = cont/len(res)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Number of correct predictions:', cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = test_data\n",
    "\n",
    "pred_data['source','weight','target']['edge_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "pred_data['source','weight','target']['edge_label'] = torch.tensor(res['weight'], dtype=torch.long)\n",
    "print(pred_data['source','weight','target']['edge_label'])\n",
    "torch.save(pred_data, 'pred_data_150.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in connected edges:     98/183 = 0.5355191256830601\n",
      "Accuracy in non connected edges: 136/170 = 0.8\n"
     ]
    }
   ],
   "source": [
    "connected_accuracy = 0.\n",
    "nonconnected_accuracy = 0.\n",
    "\n",
    "n1,n2=0,0\n",
    "ncon,nncon=0,0\n",
    "for i in res.itertuples():\n",
    "    if i.compare == 0.:\n",
    "        if i.compare == i.weight: n1+=1\n",
    "        nncon+=1\n",
    "    elif i.compare == 1.0:\n",
    "        if i.compare == i.weight: n2+=1\n",
    "        ncon+=1\n",
    "\n",
    "connected_accuracy = n2/ncon\n",
    "nonconnected_accuracy = n1/nncon\n",
    "\n",
    "print(f'Accuracy in connected edges:     {n2}/{ncon} = {connected_accuracy}')\n",
    "print(f'Accuracy in non connected edges: {n1}/{nncon} = {nonconnected_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with different graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a new csv file\n",
    "file_test = cwd + '/points_10.csv'\n",
    "df_test=pd.read_csv(file_test, sep=',')\n",
    "\n",
    "#Round the values of the dataset to 4 decimal places\n",
    "df_test = df_test.round(4)\n",
    "\n",
    "#Add a column to use as index from 0 to the length of the dataset\n",
    "df_test['n_label'] = range(0, len(df_test))\n",
    "\n",
    "#delete the column p_label\n",
    "df_test = df_test.drop('p_label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10=HeteroData()\n",
    "\n",
    "nodes_s=df_test['n_label'].values\n",
    "nodes_t=df_test['n_label'].values\n",
    "\n",
    "data_10['source'].node_id = torch.tensor(nodes_s, dtype=torch.long)\n",
    "data_10['target'].node_id = torch.tensor(nodes_t, dtype=torch.long)\n",
    "\n",
    "data_10['source'].x = Tensor(df_test[['x', 'y', 'z']].values)\n",
    "data_10['target'].x = Tensor(df_test[['x', 'y', 'z']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_test_path = cwd + '/grap_10.csv'\n",
    "\n",
    "# Importing the dataset\n",
    "df_test_edge = pd.read_csv(edge_test_path)\n",
    "\n",
    "edge_index_test = torch.tensor([df_test_edge['Source'], df_test_edge['Target']], dtype=torch.long)\n",
    "\n",
    "data_10['source', 'weight', 'target'].edge_index = edge_index_test\n",
    "\n",
    "weight_test = torch.from_numpy(df_test_edge['weight'].values).to(torch.float)\n",
    "\n",
    "data_10['source', 'weight', 'target'].edge_label=weight_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10= T.ToUndirected()(data_10)\n",
    "del data_10['target', 'rev_weight', 'source'].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={\n",
       "    node_id=[100],\n",
       "    x=[100, 3],\n",
       "  },\n",
       "  target={\n",
       "    node_id=[100],\n",
       "    x=[100, 3],\n",
       "  },\n",
       "  (source, weight, target)={\n",
       "    edge_index=[2, 107],\n",
       "    edge_label=[107],\n",
       "  },\n",
       "  (target, rev_weight, source)={ edge_index=[2, 107] }\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_10.validate(raise_on_error=True))\n",
    "data_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.4426\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    data_10 = data_10.to(device)\n",
    "    pred = model(data_10.x_dict, data_10.edge_index_dict,\n",
    "                 data_10['source', 'target'].edge_index)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    target = data_10['source', 'target'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "sour = data_10['source', 'target'].edge_index[0].cpu().numpy()\n",
    "tar = data_10['source', 'target'].edge_index[1].cpu().numpy()\n",
    "pred = pred.cpu().numpy()\n",
    "target = target.cpu().numpy()\n",
    "\n",
    "res=pd.DataFrame({'source': sour, 'target': tar, 'pred': pred, 'compare': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6542056074766355\n",
      "Number of correct predictions: 70\n"
     ]
    }
   ],
   "source": [
    "#Add a new column if pred is greater or equal than 0.5 then 1 else 0.5\n",
    "res['weight'] = np.where(res['pred']>0.5, 1, 0.5)\n",
    "\n",
    "#compare column rating_1 with target and if they are equal add up\n",
    "cont=0\n",
    "for i in res.itertuples():\n",
    "    if i.compare == i.weight:\n",
    "        cont+=1\n",
    "\n",
    "#Calculate the accuracy\n",
    "accuracy = cont/len(res)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Number of correct predictions:', cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
